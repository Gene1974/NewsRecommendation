{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# + SubVert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    " \n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True  \n",
    "session = tf.Session(config=config)\n",
    " \n",
    "KTF.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import itertools\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import keras\n",
    "from keras.utils.np_utils import *\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding, concatenate\n",
    "from keras.layers import Dense, Input, Flatten, average,Lambda\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers #keras2\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from keras.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "def trans2tsp(timestr):\n",
    "    return int(time.mktime(datetime.strptime(timestr, '%m/%d/%Y %I:%M:%S %p').timetuple()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsample(nnn,ratio):\n",
    "    if ratio >len(nnn):\n",
    "        return random.sample(nnn*(ratio//len(nnn)+1),ratio)\n",
    "    else:\n",
    "        return random.sample(nnn,ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(pn,labeler,pos):\n",
    "    index=np.arange(pn.shape[0])\n",
    "    pn=pn[index]\n",
    "    labeler=labeler[index]\n",
    "    pos=pos[index]\n",
    "    \n",
    "    for i in range(pn.shape[0]):\n",
    "        index=np.arange(npratio+1)\n",
    "        pn[i,:]=pn[i,index]\n",
    "        labeler[i,:]=labeler[i,index]\n",
    "    return pn,labeler,pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "def mrr_score(y_true, y_score):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "    return np.sum(rr_score) / np.sum(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REad News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_news(path,filenames):\n",
    "    news={}\n",
    "    category=[]\n",
    "    subcategory=[]\n",
    "    news_index={}\n",
    "    index=1\n",
    "    word_dict={}\n",
    "    word_index=1\n",
    "    with open(os.path.join(path,filenames)) as f:\n",
    "        lines=f.readlines()\n",
    "    for line in lines:\n",
    "        splited = line.strip('\\n').split('\\t')\n",
    "        doc_id,vert,subvert,title= splited[0:4]\n",
    "        news_index[doc_id]=index\n",
    "        index+=1\n",
    "        category.append(vert)\n",
    "        subcategory.append(subvert)\n",
    "        title = title.lower()\n",
    "        title=word_tokenize(title)\n",
    "        news[doc_id]=[vert,subvert,title]\n",
    "        for word in title:\n",
    "            word = word.lower()\n",
    "            if not(word in word_dict):\n",
    "                word_dict[word]=word_index\n",
    "                word_index+=1\n",
    "    category=list(set(category))\n",
    "    subcategory=list(set(subcategory))\n",
    "    category_dict={}\n",
    "    index=1\n",
    "    for c in category:\n",
    "        category_dict[c]=index\n",
    "        index+=1\n",
    "    subcategory_dict={}\n",
    "    index=1\n",
    "    for c in subcategory:\n",
    "        subcategory_dict[c]=index\n",
    "        index+=1\n",
    "    return news,news_index,category_dict,subcategory_dict,word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_path = '/data/qit16/MIND-Full/'\n",
    "embedding_path = '/data/qit16/word_embedding/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.8 s, sys: 880 ms, total: 26.7 s\n",
      "Wall time: 26.7 s\n"
     ]
    }
   ],
   "source": [
    "%time news,news_index,category_dict,subcategory_dict,word_dict = read_news(data_root_path,'docs.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_input(news,news_index,category,subcategory,word_dict):\n",
    "    news_num=len(news)+1\n",
    "    news_title=np.zeros((news_num,MAX_SENTENCE),dtype='int32')\n",
    "    news_vert=np.zeros((news_num,),dtype='int32')\n",
    "    news_subvert=np.zeros((news_num,),dtype='int32')\n",
    "    for key in news:    \n",
    "        vert,subvert,title=news[key]\n",
    "        doc_index=news_index[key]\n",
    "        news_vert[doc_index]=category[vert]\n",
    "        news_subvert[doc_index]=subcategory[subvert]\n",
    "        for word_id in range(min(MAX_SENTENCE,len(title))):\n",
    "            news_title[doc_index,word_id]=word_dict[title[word_id].lower()]\n",
    "        \n",
    "    return news_title,news_vert,news_subvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_title,news_vert,news_subvert=get_doc_input(news,news_index,category_dict,subcategory_dict,word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_matrix(embedding_path,word_dict):\n",
    "    embedding_matrix = np.zeros((len(word_dict)+1,300))\n",
    "    have_word=[]\n",
    "    with open(os.path.join(embedding_path,'glove.840B.300d.txt'),'rb') as f:\n",
    "        while True:\n",
    "            l=f.readline()\n",
    "            if len(l)==0:\n",
    "                break\n",
    "            l=l.split()\n",
    "            word = l[0].decode()\n",
    "            if word in word_dict:\n",
    "                index = word_dict[word]\n",
    "                tp = [float(x) for x in l[1:]]\n",
    "                embedding_matrix[index]=np.array(tp)\n",
    "                have_word.append(word)\n",
    "    return embedding_matrix,have_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#End Read News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_clickhistory(path,filename):\n",
    "    \n",
    "    lines = []\n",
    "    userids = []\n",
    "    with open(os.path.join(data_root_path,filename)) as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    sessions = []\n",
    "    for i in range(len(lines)):\n",
    "        if 'Full' in data_root_path:\n",
    "            _,click, imp = lines[i].strip().split('\\t')\n",
    "        else:\n",
    "            _,_,click, imp = lines[i].strip().split('\\t')\n",
    "        clikcs = click.split('#N#')\n",
    "        true_click = []\n",
    "        for click in clikcs:\n",
    "            t = click.split('#TAB#')[0]\n",
    "            if t =='':\n",
    "                continue\n",
    "            true_click.append(t)\n",
    "        pos, neg, _ = imp.split('#TAB#')\n",
    "        pos = pos.split()\n",
    "        neg = neg.split()\n",
    "        sessions.append([true_click,pos,neg])\n",
    "    return sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_session = read_clickhistory(data_root_path,'train_sam2.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_part_clickhistory(path,filename,num=200000):\n",
    "    \n",
    "    lines = []\n",
    "    userids = []\n",
    "    with open(os.path.join(data_root_path,filename)) as f:\n",
    "        for i in range(num):\n",
    "            l = f.readline()\n",
    "            lines.append(l)\n",
    "    print(num)\n",
    "        \n",
    "    sessions = []\n",
    "    for i in range(len(lines)):\n",
    "        if 'Full' in data_root_path:\n",
    "            _,click, imp = lines[i].strip().split('\\t')\n",
    "        else:\n",
    "            _,_,click, imp = lines[i].strip().split('\\t')\n",
    "        clikcs = click.split('#N#')\n",
    "        true_click = []\n",
    "        for click in clikcs:\n",
    "            t = click.split('#TAB#')[0]\n",
    "            if t =='':\n",
    "                continue\n",
    "            true_click.append(t)\n",
    "        pos, neg, _ = imp.split('#TAB#')\n",
    "        pos = pos.split()\n",
    "        neg = neg.split()\n",
    "        sessions.append([true_click,pos,neg])\n",
    "    return sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "test_session = read_part_clickhistory(data_root_path,'test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_session = read_clickhistory(data_root_path,'val_sam2.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parser User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ALL = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_user(session):\n",
    "    user_num = len(session)\n",
    "    user={'click': np.zeros((user_num,MAX_ALL),dtype='int32'),}\n",
    "    for user_id in range(len(session)):\n",
    "        tclick = []\n",
    "        click, pos, neg =session[user_id]\n",
    "        for i in range(len(click)):\n",
    "            tclick.append(news_index[click[i]])\n",
    "        click = tclick\n",
    "\n",
    "        if len(click) >MAX_ALL:\n",
    "            click = click[-MAX_ALL:]\n",
    "        else:\n",
    "            click=[0]*(MAX_ALL-len(click)) + click\n",
    "            \n",
    "        user['click'][user_id] = np.array(click)\n",
    "    return user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_user = parse_user(train_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user = parse_user(test_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_user = parse_user(val_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "npratio=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_input(session):\n",
    "    sess_pos = []\n",
    "    sess_neg = []\n",
    "    user_id = []\n",
    "    for sess_id in range(len(session)):\n",
    "        sess = session[sess_id]\n",
    "        _, poss, negs=sess\n",
    "        for i in range(len(poss)):\n",
    "            pos = poss[i]\n",
    "            neg=newsample(negs,npratio)\n",
    "            sess_pos.append(pos)\n",
    "            sess_neg.append(neg)\n",
    "            user_id.append(sess_id)\n",
    "    print(len(user_id))\n",
    "    sess_all = np.zeros((len(sess_pos),1+npratio),dtype='int32')\n",
    "    label = np.zeros((len(sess_pos),1+npratio))\n",
    "    for sess_id in range(sess_all.shape[0]):\n",
    "        pos = sess_pos[sess_id]\n",
    "        negs = sess_neg[sess_id]\n",
    "        sess_all[sess_id,0] = news_index[pos]\n",
    "        index = 1\n",
    "        for neg in negs:\n",
    "            sess_all[sess_id,index] = news_index[neg]\n",
    "            index+=1\n",
    "        #index = np.random.randint(1+npratio)\n",
    "        label[sess_id,0]=1\n",
    "    user_id = np.array(user_id, dtype='int32')\n",
    "    \n",
    "    return sess_all, user_id, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3383654\n"
     ]
    }
   ],
   "source": [
    "train_sess, train_user_id, train_label = get_train_input(train_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_input(session):\n",
    "    \n",
    "    Impressions = []\n",
    "    userid = []\n",
    "    for sess_id in range(len(session)):\n",
    "        _, poss, negs = session[sess_id]\n",
    "        imp = {'labels':[],\n",
    "                'docs':[]}\n",
    "        userid.append(sess_id)\n",
    "        for i in range(len(poss)):\n",
    "            docid = news_index[poss[i]]\n",
    "            imp['docs'].append(docid)\n",
    "            imp['labels'].append(1)\n",
    "        for i in range(len(negs)):\n",
    "            docid = news_index[negs[i]]\n",
    "            imp['docs'].append(docid)\n",
    "            imp['labels'].append(0)\n",
    "        Impressions.append(imp)\n",
    "        \n",
    "    userid = np.array(userid,dtype='int32')\n",
    "    \n",
    "    return Impressions, userid,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_impressions, test_userids = get_test_input(test_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "KG_root_path = '/data/qit16/Opendata_KG/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(KG_root_path,'V21UrlDocs22_all_josn.tsv')) as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "entity_dict = {}\n",
    "news_entity = np.zeros((len(news_index)+1,5),dtype='int32')\n",
    "entity_index = 1\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    data = json.loads(lines[i].strip('\\n'))\n",
    "    nix = news_index[data['doc_id']]\n",
    "    ct = 0\n",
    "    for e in data['entities']:\n",
    "        eid = e['WikidataId']\n",
    "        if not eid in entity_dict:\n",
    "            entity_dict[eid] = entity_index\n",
    "            entity_index += 1\n",
    "            \n",
    "        news_entity[nix,ct] = entity_dict[eid]\n",
    "        ct += 1\n",
    "        if ct == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(KG_root_path,'description.txt')) as f:\n",
    "#     lines = f.readlines()\n",
    "    \n",
    "# entity_desc = {}\n",
    "# for i in range(len(lines)):\n",
    "#     eid,text = lines[i].strip('\\n').split('\\t')\n",
    "#     if not eid in entity_dict:\n",
    "#         continue\n",
    "#     entity_desc[eid] = text\n",
    "    \n",
    "# for nid in entity_dict:\n",
    "#     if not nid in entity_desc:\n",
    "#         entity_desc[nid] = ''\n",
    "        \n",
    "# with open('entity_desc.json','w') as f:\n",
    "#     json.dump(entity_desc,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entity_desc.json','r') as f:\n",
    "    entity_desc_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64963"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_dict = {}\n",
    "word_index = len(word_dict)+1\n",
    "\n",
    "entity_desc = np.zeros((len(entity_dict)+1,10),dtype='int32')\n",
    "\n",
    "for eid in entity_desc_dict:\n",
    "    text = entity_desc_dict[eid]\n",
    "    text = word_tokenize(text.lower())\n",
    "    for i in range(min(len(text),10)):\n",
    "        word = text[i]\n",
    "        if not word in word_dict:\n",
    "            word_dict[word] = word_index\n",
    "            word_index += 1\n",
    "        entity_desc[entity_dict[eid],i] = word_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70819"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_word_embedding_matrix, have_word = load_matrix(embedding_path,word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_word_embedding_matrix, have_word = load_matrix(embedding_path,entity_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsFetcher():\n",
    "    def __init__(self,news_title,news_entity,entity_desc):\n",
    "        self.news_title = news_title\n",
    "        self.news_entity = news_entity\n",
    "        self.entity_desc = entity_desc\n",
    "        \n",
    "    def fetch(self,docids):\n",
    "        bz,n = docids.shape\n",
    "        news_title = self.news_title[docids] #(N,30)\n",
    "        news_entity_ids = self.news_entity[docids]\n",
    "        news_entity_texts = self.entity_desc[news_entity_ids] #(N,30,10)\n",
    "        news_entity_texts = news_entity_texts.reshape((bz,n,-1))\n",
    "        \n",
    "        news_info = np.concatenate([news_title,news_entity_texts],axis=-1)\n",
    "        \n",
    "        return news_info\n",
    "    \n",
    "    def fetch_dim1(self,docids):\n",
    "        bz, = docids.shape\n",
    "        news_title = self.news_title[docids] #(N,30)\n",
    "        news_entity_ids = self.news_entity[docids]\n",
    "        news_entity_texts = self.entity_desc[news_entity_ids] #(N,30,10)\n",
    "        news_entity_texts = news_entity_texts.reshape((bz,-1))\n",
    "        \n",
    "        news_info = np.concatenate([news_title,news_entity_texts],axis=-1)\n",
    "        \n",
    "        return news_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_fetcher = NewsFetcher(news_title,news_entity,entity_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "\n",
    "class get_hir_train_generator(Sequence):\n",
    "    def __init__(self,news_fetcher,clicked_news,user_id, news_id, label, batch_size):\n",
    "        self.news_fetcher = news_fetcher\n",
    "        self.clicked_news = clicked_news\n",
    "\n",
    "        self.user_id = user_id\n",
    "        self.doc_id = news_id\n",
    "        self.label = label\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.ImpNum = self.label.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.ImpNum / float(self.batch_size)))\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx*self.batch_size\n",
    "        ed = (idx+1)*self.batch_size\n",
    "        if ed> self.ImpNum:\n",
    "            ed = self.ImpNum\n",
    "        label = self.label[start:ed]\n",
    "        \n",
    "        doc_ids = self.doc_id[start:ed]\n",
    "        info= self.news_fetcher.fetch(doc_ids)\n",
    "        \n",
    "        user_ids = self.user_id[start:ed]\n",
    "        clicked_ids = self.clicked_news[user_ids]\n",
    "        user_info = self.news_fetcher.fetch(clicked_ids)\n",
    "        \n",
    "        click_mask = clicked_ids>0\n",
    "        click_mask = np.array(click_mask,dtype='float32')\n",
    "        \n",
    "\n",
    "        return ([info, user_info],[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = get_hir_train_generator(news_fetcher,train_user['click'],train_user_id,train_sess,train_label,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_hir_user_generator(Sequence):\n",
    "    def __init__(self,news_emb,clicked_news,batch_size):\n",
    "        self.news_emb = news_emb\n",
    "\n",
    "        self.clicked_news = clicked_news\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.ImpNum = self.clicked_news.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.ImpNum / float(self.batch_size)))\n",
    "\n",
    "    \n",
    "    def __get_news(self,docids):\n",
    "        news_emb = self.news_emb[docids]\n",
    "\n",
    "        return news_emb\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = idx*self.batch_size\n",
    "        ed = (idx+1)*self.batch_size\n",
    "        if ed> self.ImpNum:\n",
    "            ed = self.ImpNum\n",
    "            \n",
    "        clicked_ids = self.clicked_news[start:ed]\n",
    "        user_title = self.__get_news(clicked_ids)\n",
    "\n",
    "        \n",
    "        return user_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_hir_news_generator(Sequence):\n",
    "    def __init__(self,news_fetcher,batch_size):\n",
    "        self.news_fetcher = news_fetcher\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.ImpNum = len(news_index)+1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.ImpNum / float(self.batch_size)))\n",
    "\n",
    "    \n",
    "    def __get_news(self,docids):\n",
    "        news_emb = self.news_emb[docids]\n",
    "\n",
    "        return news_emb\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = idx*self.batch_size\n",
    "        ed = (idx+1)*self.batch_size\n",
    "        if ed> self.ImpNum:\n",
    "            ed = self.ImpNum\n",
    "            \n",
    "        docids = np.array([i for i in range(start,ed)])\n",
    "            \n",
    "        info = self.news_fetcher.fetch_dim1(docids)\n",
    "\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_generator = get_hir_news_generator(news_fetcher,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_impressions,user_scoring):\n",
    "    AUC = []\n",
    "    MRR = []\n",
    "    nDCG5 = []\n",
    "    nDCG10 = []\n",
    "    for i in range(len(test_impressions)):\n",
    "        labels = test_impressions[i]['labels']\n",
    "        nids = test_impressions[i]['docs']\n",
    "\n",
    "        uv = user_scoring[i]\n",
    "\n",
    "        nvs = news_scoring[nids]\n",
    "        score = np.dot(nvs,uv)\n",
    "\n",
    "        auc = roc_auc_score(labels,score)\n",
    "        mrr = mrr_score(labels,score)\n",
    "        ndcg5 = ndcg_score(labels,score,k=5)\n",
    "        ndcg10 = ndcg_score(labels,score,k=10)\n",
    "    \n",
    "        AUC.append(auc)\n",
    "        MRR.append(mrr)\n",
    "        nDCG5.append(ndcg5)\n",
    "        nDCG10.append(ndcg10)\n",
    "        \n",
    "    AUC = np.array(AUC).mean()\n",
    "    MRR = np.array(MRR).mean()\n",
    "    nDCG5 = np.array(nDCG5).mean()\n",
    "    nDCG10 = np.array(nDCG10).mean()\n",
    "\n",
    "    \n",
    "    return AUC, MRR, nDCG5, nDCG10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    " \n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ',\n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK',\n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV',\n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    " \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    " \n",
    "    def call(self, x):\n",
    "        #如果只传入Q_seq,K_seq,V_seq，那么就不做Mask\n",
    "        #如果同时传入Q_seq,K_seq,V_seq,Q_len,V_len，那么对多余部分做Mask\n",
    "        if len(x) == 3:\n",
    "            Q_seq,K_seq,V_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        #对Q、K、V做线性变换\n",
    "        seq_length = 10\n",
    "        \n",
    "        Q_seq = K.dot(Q_seq, self.WQ)\n",
    "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
    "        #计算内积，然后mask，然后softmax\n",
    "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = K.softmax(A)\n",
    "        #输出并mask\n",
    "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n",
    "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n",
    "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH=30\n",
    "MAX_SENTS=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AttentivePooling(dim1,dim2):\n",
    "    vecs_input = Input(shape=(dim1,dim2),dtype='float32') # 50, 400\n",
    "    user_vecs =Dropout(0.2)(vecs_input)\n",
    "    user_att = Dense(200,activation='tanh')(user_vecs) # 50, 200\n",
    "    user_att = keras.layers.Flatten()(Dense(1)(user_att)) # 50,\n",
    "    user_att = Activation('softmax')(user_att)\n",
    "    user_vec = keras.layers.Dot((1,1))([user_vecs,user_att])\n",
    "    model = Model(vecs_input,user_vec)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AttentivePoolingAMask(dim1,dim2):\n",
    "    vecs_input = Input(shape=(dim1,dim2),dtype='float32')\n",
    "    mask_input = Input(shape=(dim1,),dtype='float32')\n",
    "\n",
    "    user_vecs =Dropout(0.2)(vecs_input)\n",
    "    user_att = Dense(200,activation='tanh')(user_vecs)\n",
    "    user_att = keras.layers.Flatten()(Dense(1)(user_att))\n",
    "    user_att = keras.layers.Lambda(lambda x: x[0]-100*(1-x[1]))([user_att,mask_input])\n",
    "    user_att = Activation('softmax')(user_att)\n",
    "    user_vec = keras.layers.Dot((1,1))([user_vecs,user_att])\n",
    "    model = Model([vecs_input,mask_input],user_vec)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_encoder(mode,length,word_embedding_layer):\n",
    "\n",
    "    sentence_input = Input(shape=(length,),dtype='int32')\n",
    "    \n",
    "    #word_embedding_layer = Embedding(word_embedding_matrix.shape[0], 300, weights=[word_embedding_matrix],trainable=True)\n",
    "    word_vecs = word_embedding_layer(sentence_input)\n",
    "    droped_vecs = Dropout(0.2)(word_vecs)\n",
    "    \n",
    "    if 'SelfAtt' in mode:\n",
    "        word_rep = Attention(20,20)([droped_vecs]*3)\n",
    "    elif 'CNN' in mode:\n",
    "        word_rep = Conv1D(400,3,padding='same')(droped_vecs)\n",
    "    \n",
    "    if 'ReLU' in mode:\n",
    "        print('use relu')\n",
    "        word_rep = Activation('relu')(word_rep)\n",
    "    \n",
    "    droped_rep = Dropout(0.2)(word_rep)    \n",
    "    title_vec = AttentivePooling(length,400)(droped_rep)\n",
    "\n",
    "    sentEncodert = Model(sentence_input, title_vec)\n",
    "    return sentEncodert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_encoder(mode):\n",
    "    # shape=(50,30+5*10,)\n",
    "    news_input = Input(shape=(30+5*10,),dtype='int32')\n",
    "    \n",
    "    title_input = Lambda(lambda x:x[:,:30])(news_input)\n",
    "    entity_input = Lambda(lambda x:x[:,30:])(news_input) #(bz,5*10)\n",
    "    entity_input = Reshape((5,10))(entity_input)\n",
    "\n",
    "    word_embedding_layer = Embedding(title_word_embedding_matrix.shape[0], 300, weights=[title_word_embedding_matrix],trainable=True)\n",
    "    \n",
    "    title_encoder = get_text_encoder(mode,30,word_embedding_layer)\n",
    "\n",
    "    vec = title_encoder(title_input)\n",
    "\n",
    "\n",
    "    sentEncodert = Model(news_input, vec)\n",
    "    return sentEncodert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_encoder(mode,user_relu):\n",
    "    user_vecs_input = Input(shape=(50,400))    \n",
    "    click_mask_input = Input(shape=(50,))\n",
    "    \n",
    "    user_vecs = Dropout(0.2)(user_vecs_input)\n",
    "\n",
    "    if mode=='NRMS':\n",
    "        user_vecs = Attention(20,20)([user_vecs]*3)\n",
    "        user_vec = AttentivePooling(50,400)(user_vecs)\n",
    "    elif mode in ['NAML',]:\n",
    "        user_vec = AttentivePooling(50,400)(user_vecs)     \n",
    "    elif mode in ['NAML-Dense']:\n",
    "        user_vec = AttentivePooling(50,400)(user_vecs)     \n",
    "        user_vec = Dense(400)(user_vec)\n",
    "    if user_relu:\n",
    "        user_vec = Activation('relu')(user_vec)\n",
    "        \n",
    "    return Model(user_vecs_input,user_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_TITLE = 768\n",
    "def create_model(mode1,mode2,user_relu):\n",
    "    \n",
    "    MAX_LENGTH = 30    \n",
    "    \n",
    "    news_encoder = get_news_encoder(mode1)\n",
    "    user_encoder = get_user_encoder(mode2,user_relu)\n",
    "    \n",
    "    clicked_title_input = Input(shape=(50,30+5*10,), dtype='int32')    \n",
    "    title_inputs = Input(shape=(1+npratio,30+5*10,),dtype='int32') \n",
    "\n",
    "    user_vecs = TimeDistributed(news_encoder)(clicked_title_input)\n",
    "    user_vec = user_encoder(user_vecs)\n",
    "    \n",
    "    news_vecs = TimeDistributed(news_encoder)(title_inputs)\n",
    "    news_vecs = Dropout(0.2)(news_vecs)\n",
    "    \n",
    "    scores = keras.layers.Dot(axes=-1)([news_vecs,user_vec])\n",
    "    \n",
    "    logits = keras.layers.Activation(keras.activations.softmax,name = 'recommend')(scores)     \n",
    "\n",
    "    model = Model([title_inputs, clicked_title_input,],logits) # max prob_click_positive\n",
    "    model.compile(loss=['categorical_crossentropy'],\n",
    "                  optimizer=Adam(lr=0.0001), \n",
    "                  #optimizer= SGD(lr=0.01),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    return model,news_encoder,user_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Res = []\n",
    "news_encoder_mode = 'SelfAtt-ReLU'\n",
    "user_encoder_mode = 'NRMS'\n",
    "\n",
    "model,news_encoder,user_encoder, = create_model(news_encoder_mode,user_encoder_mode)\n",
    "# model.fit_generator(train_generator,epochs=1)\n",
    "\n",
    "model.fit_generator(train_generator,epochs=2)\n",
    "news_scoring = news_encoder.predict_generator(news_generator,verbose=1)\n",
    "test_user_generator = get_hir_user_generator(news_scoring,test_user['click'][:210000],32)\n",
    "test_user_scoring = user_encoder.predict_generator(test_user_generator,verbose=1)\n",
    "g2 = evaluate(test_impressions,test_user_scoring)\n",
    "Res.append(g2)\n",
    "    \n",
    "print(g2,Res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
