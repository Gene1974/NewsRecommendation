{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:33:59.779352Z",
     "start_time": "2022-04-21T00:33:59.774147Z"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# + SubVert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:34:02.085683Z",
     "start_time": "2022-04-21T00:34:02.081089Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    " \n",
    "# config = tf.ConfigProto()  \n",
    "# config.gpu_options.allow_growth=True  \n",
    "# session = tf.Session(config=config)\n",
    " \n",
    "# KTF.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:34:11.022486Z",
     "start_time": "2022-04-21T00:34:10.342636Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import itertools\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:34:11.401453Z",
     "start_time": "2022-04-21T00:34:11.194229Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:53:05.402892Z",
     "start_time": "2022-04-21T00:53:05.392632Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils.np_utils import *\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding, concatenate\n",
    "from keras.layers import Dense, Input, Flatten, average,Lambda\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer, InputSpec\n",
    "from keras import initializers #keras2\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from keras.optimizers import *\n",
    "\n",
    "from tensorflow.keras.utils import Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:34:25.347932Z",
     "start_time": "2022-04-21T00:34:25.342332Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "def trans2tsp(timestr):\n",
    "    return int(time.mktime(datetime.strptime(timestr, '%m/%d/%Y %I:%M:%S %p').timetuple()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:34:25.792264Z",
     "start_time": "2022-04-21T00:34:25.786510Z"
    }
   },
   "outputs": [],
   "source": [
    "def newsample(nnn,ratio):\n",
    "    if ratio >len(nnn):\n",
    "        return random.sample(nnn*(ratio//len(nnn)+1),ratio)\n",
    "    else:\n",
    "        return random.sample(nnn,ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:34:26.364939Z",
     "start_time": "2022-04-21T00:34:26.356937Z"
    }
   },
   "outputs": [],
   "source": [
    "def shuffle(pn,labeler,pos):\n",
    "    index=np.arange(pn.shape[0])\n",
    "    pn=pn[index]\n",
    "    labeler=labeler[index]\n",
    "    pos=pos[index]\n",
    "    \n",
    "    for i in range(pn.shape[0]):\n",
    "        index=np.arange(npratio+1)\n",
    "        pn[i,:]=pn[i,index]\n",
    "        labeler[i,:]=labeler[i,index]\n",
    "    return pn,labeler,pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:34:27.697277Z",
     "start_time": "2022-04-21T00:34:27.685977Z"
    }
   },
   "outputs": [],
   "source": [
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "def mrr_score(y_true, y_score):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "    return np.sum(rr_score) / np.sum(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:31:39.174791Z",
     "start_time": "2022-04-21T00:31:38.911242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65238\n"
     ]
    }
   ],
   "source": [
    "news_set = set()\n",
    "news = []\n",
    "with open('/data/Recommend/MIND/MINDsmall_train/news.tsv', 'r') as f:\n",
    "    for line in f:\n",
    "        data = line.split('\\t')\n",
    "        news_id = data[0]\n",
    "        if news_id not in news_set:\n",
    "            news.append(line)\n",
    "            news_set.add(news_id)\n",
    "with open('/data/Recommend/MIND/MINDsmall_dev/news.tsv') as f:\n",
    "    for line in f:\n",
    "        data = line.split('\\t')\n",
    "        news_id = data[0]\n",
    "        if news_id not in news_set:\n",
    "            news.append(line)\n",
    "            news_set.add(news_id)\n",
    "# with open('/data/Recommend/MIND/MINDlarge_test/news.tsv') as f:\n",
    "#     for line in f:\n",
    "#         data = line.split('\\t')\n",
    "#         news_id = data[0]\n",
    "#         if news_id not in news_set:\n",
    "#             news.append(line)\n",
    "#             news_set.add(news_id)\n",
    "\n",
    "# with open('/data/Recommend/MIND/small_news.tsv', 'w') as f:\n",
    "#     f.writelines(news)\n",
    "\n",
    "print(len(news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REad News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:34:32.519869Z",
     "start_time": "2022-04-21T00:34:32.503053Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_news(path,filenames):\n",
    "    news={}\n",
    "    category=[]\n",
    "    subcategory=[]\n",
    "    news_index={}\n",
    "    index=1\n",
    "    word_dict={}\n",
    "    word_index=1\n",
    "    body_word_dict = {}\n",
    "    with open(os.path.join(path,filenames)) as f:\n",
    "        lines=f.readlines()\n",
    "    for line in lines:\n",
    "        splited = line.strip('\\n').split('\\t')\n",
    "        doc_id,vert,subvert,title= splited[0:4]\n",
    "        news_index[doc_id]=index\n",
    "        index+=1\n",
    "        category.append(vert)\n",
    "        subcategory.append(subvert)\n",
    "        title = title.lower()\n",
    "        title=word_tokenize(title)\n",
    "        #body = release2private[doc_id]\n",
    "        #body = body.split()[:200]\n",
    "        body = None\n",
    "        news[doc_id]=[vert,subvert,title,body]\n",
    "        for word in title:\n",
    "            word = word.lower()\n",
    "            if not(word in word_dict):\n",
    "                word_dict[word]=word_index\n",
    "                word_index+=1\n",
    "        \n",
    "    category=list(set(category))\n",
    "    subcategory=list(set(subcategory))\n",
    "    category_dict={}\n",
    "    index=1\n",
    "    for c in category:\n",
    "        category_dict[c]=index\n",
    "        index+=1\n",
    "    subcategory_dict={}\n",
    "    index=1\n",
    "    for c in subcategory:\n",
    "        subcategory_dict[c]=index\n",
    "        index+=1\n",
    "        \n",
    "    for word in body_word_dict:\n",
    "        if word in word_dict:\n",
    "            continue\n",
    "        if body_word_dict[word]<3:\n",
    "            continue\n",
    "        word_dict[word] = word_index\n",
    "        word_index += 1\n",
    "        \n",
    "    return news,news_index,category_dict,subcategory_dict,word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:34:34.809512Z",
     "start_time": "2022-04-21T00:34:34.805464Z"
    }
   },
   "outputs": [],
   "source": [
    "# data_root_path = '/data1/qitao/MIND-Full-Release/'\n",
    "# embedding_path = '/data/data/qit/'\n",
    "data_root_path = '/data/Recommend/MIND/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:36:15.421573Z",
     "start_time": "2022-04-21T00:36:07.387401Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.89 s, sys: 143 ms, total: 8.03 s\n",
      "Wall time: 8.03 s\n"
     ]
    }
   ],
   "source": [
    "# %time news,news_index,category_dict,subcategory_dict,word_dict = read_news(data_root_path,'docs.tsv')\n",
    "%time news,news_index,category_dict,subcategory_dict,word_dict = read_news(data_root_path,'small_news.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:36:56.606299Z",
     "start_time": "2022-04-21T00:36:56.601879Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:36:59.608405Z",
     "start_time": "2022-04-21T00:36:59.594635Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_doc_input(news,news_index,category,subcategory,word_dict):\n",
    "    news_num=len(news)+1\n",
    "    news_title=np.zeros((news_num,MAX_SENTENCE),dtype='int32')\n",
    "    news_body=None\n",
    "    news_vert=np.zeros((news_num,),dtype='int32')\n",
    "    news_subvert=np.zeros((news_num,),dtype='int32')\n",
    "    for key in news:    \n",
    "        vert,subvert,title,body=news[key]\n",
    "        doc_index=news_index[key]\n",
    "        news_vert[doc_index]=category[vert]\n",
    "        news_subvert[doc_index]=subcategory[subvert]\n",
    "        for word_id in range(min(MAX_SENTENCE,len(title))):\n",
    "            news_title[doc_index,word_id]=word_dict[title[word_id].lower()]\n",
    "        \n",
    "    return news_title,news_vert,news_subvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:37:41.048261Z",
     "start_time": "2022-04-21T00:37:40.679085Z"
    }
   },
   "outputs": [],
   "source": [
    "news_title,news_vert,news_subvert=get_doc_input(news,news_index,category_dict,subcategory_dict,word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:37:47.194585Z",
     "start_time": "2022-04-21T00:37:47.190421Z"
    }
   },
   "outputs": [],
   "source": [
    "news_info = news_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:38:35.905811Z",
     "start_time": "2022-04-21T00:38:35.895340Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_path = '/data/pretrained/Glove/'\n",
    "def load_matrix(embedding_path,word_dict):\n",
    "    embedding_matrix = np.zeros((len(word_dict)+1,300))\n",
    "    have_word=[]\n",
    "    with open(os.path.join(embedding_path,'glove.840B.300d.txt'),'rb') as f:\n",
    "        while True:\n",
    "            l=f.readline()\n",
    "            if len(l)==0:\n",
    "                break\n",
    "            l=l.split()\n",
    "            word = l[0].decode()\n",
    "            if word in word_dict:\n",
    "                index = word_dict[word]\n",
    "                tp = [float(x) for x in l[1:]]\n",
    "                embedding_matrix[index]=np.array(tp)\n",
    "                have_word.append(word)\n",
    "    return embedding_matrix,have_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:39:12.484678Z",
     "start_time": "2022-04-21T00:38:40.135337Z"
    }
   },
   "outputs": [],
   "source": [
    "title_word_embedding_matrix, have_word = load_matrix(embedding_path,word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#End Read News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:41:51.023203Z",
     "start_time": "2022-04-21T00:41:51.010889Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_clickhistory(path,filename):\n",
    "    \n",
    "    lines = []\n",
    "    userids = []\n",
    "    with open(os.path.join(data_root_path,filename)) as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    sessions = []\n",
    "    for i in range(len(lines)):\n",
    "        _,uid,eventime, click, imps = lines[i].strip().split('\\t')\n",
    "        if click == '':\n",
    "            clicks = []\n",
    "        else:\n",
    "            clikcs = click.split()\n",
    "        true_click = []\n",
    "        for click in clikcs:\n",
    "            if not click in news_index:\n",
    "                continue\n",
    "            true_click.append(click)\n",
    "        pos = []\n",
    "        neg = []\n",
    "        for imp in imps.split():\n",
    "            docid, label = imp.split('-')\n",
    "            if label == '1':\n",
    "                pos.append(docid)\n",
    "            else:\n",
    "                neg.append(docid)\n",
    "        sessions.append([true_click,pos,neg])\n",
    "    return sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:41:57.722022Z",
     "start_time": "2022-04-21T00:41:52.036753Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_session = read_clickhistory(data_root_path,'train_sam2.tsv')\n",
    "train_session = read_clickhistory(data_root_path,'MINDsmall_train/behaviors.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:42:20.871400Z",
     "start_time": "2022-04-21T00:42:20.858573Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_part_clickhistory(path,filename):\n",
    "    \n",
    "    lines = []\n",
    "    userids = []\n",
    "    with open(os.path.join(data_root_path,filename)) as f:\n",
    "        lines = f.readlines()\n",
    "    lines = lines[:100000]\n",
    "        \n",
    "    sessions = []\n",
    "    for i in range(len(lines)):\n",
    "        _,uid,eventime, click, imps = lines[i].strip().split('\\t')\n",
    "        if click == '':\n",
    "            clicks = []\n",
    "        else:\n",
    "            clikcs = click.split()\n",
    "        true_click = []\n",
    "        for click in clikcs:\n",
    "            if not click in news_index:\n",
    "                continue\n",
    "            true_click.append(click)\n",
    "        pos = []\n",
    "        neg = []\n",
    "        for imp in imps.split():\n",
    "            docid, label = imp.split('-')\n",
    "            if label == '1':\n",
    "                pos.append(docid)\n",
    "            else:\n",
    "                neg.append(docid)\n",
    "        sessions.append([true_click,pos,neg])\n",
    "    return sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:42:25.057724Z",
     "start_time": "2022-04-21T00:42:21.580192Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_session = read_part_clickhistory(data_root_path,'val_sam2.tsv')\n",
    "test_session = read_part_clickhistory(data_root_path,'MINDsmall_train/behaviors.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: 无法访问/data/qit16/MIND-Large-Release/: 没有那个文件或目录\r\n"
     ]
    }
   ],
   "source": [
    "ls /data/qit16/MIND-Large-Release/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parser User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:48:40.722222Z",
     "start_time": "2022-04-21T00:48:40.717505Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_ALL = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:48:41.975469Z",
     "start_time": "2022-04-21T00:48:41.965108Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_user(session):\n",
    "    user_num = len(session)\n",
    "    user={'click': np.zeros((user_num,MAX_ALL),dtype='int32'),}\n",
    "    for user_id in range(len(session)):\n",
    "        tclick = []\n",
    "        click, pos, neg =session[user_id]\n",
    "        for i in range(len(click)):\n",
    "            tclick.append(news_index[click[i]])\n",
    "        click = tclick\n",
    "\n",
    "        if len(click) >MAX_ALL:\n",
    "            click = click[-MAX_ALL:]\n",
    "        else:\n",
    "            click=[0]*(MAX_ALL-len(click)) + click\n",
    "            \n",
    "        user['click'][user_id] = np.array(click)\n",
    "    return user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:48:46.181006Z",
     "start_time": "2022-04-21T00:48:44.212250Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_user = parse_user(train_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:48:50.004449Z",
     "start_time": "2022-04-21T00:48:48.713789Z"
    }
   },
   "outputs": [],
   "source": [
    "test_user = parse_user(test_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:48:54.979986Z",
     "start_time": "2022-04-21T00:48:54.975693Z"
    }
   },
   "outputs": [],
   "source": [
    "npratio=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:48:55.839633Z",
     "start_time": "2022-04-21T00:48:55.825843Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_input(session):\n",
    "    sess_pos = []\n",
    "    sess_neg = []\n",
    "    user_id = []\n",
    "    for sess_id in range(len(session)):\n",
    "        sess = session[sess_id]\n",
    "        _, poss, negs=sess\n",
    "        for i in range(len(poss)):\n",
    "            pos = poss[i]\n",
    "            neg=newsample(negs,npratio)\n",
    "            sess_pos.append(pos)\n",
    "            sess_neg.append(neg)\n",
    "            user_id.append(sess_id)\n",
    "    print(len(user_id))\n",
    "    sess_all = np.zeros((len(sess_pos),1+npratio),dtype='int32')\n",
    "    label = np.zeros((len(sess_pos),1+npratio))\n",
    "    for sess_id in range(sess_all.shape[0]):\n",
    "        pos = sess_pos[sess_id]\n",
    "        negs = sess_neg[sess_id]\n",
    "        sess_all[sess_id,0] = news_index[pos]\n",
    "        index = 1\n",
    "        for neg in negs:\n",
    "            sess_all[sess_id,index] = news_index[neg]\n",
    "            index+=1\n",
    "        #index = np.random.randint(1+npratio)\n",
    "        label[sess_id,0]=1\n",
    "    user_id = np.array(user_id, dtype='int32')\n",
    "    \n",
    "    return sess_all, user_id, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:49:03.418279Z",
     "start_time": "2022-04-21T00:49:00.792592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236344\n"
     ]
    }
   ],
   "source": [
    "train_sess, train_user_id, train_label = get_train_input(train_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:49:07.705833Z",
     "start_time": "2022-04-21T00:49:07.694717Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_test_input(session):\n",
    "    \n",
    "    Impressions = []\n",
    "    userid = []\n",
    "    for sess_id in range(len(session)):\n",
    "        _, poss, negs = session[sess_id]\n",
    "        imp = {'labels':[],\n",
    "                'docs':[]}\n",
    "        userid.append(sess_id)\n",
    "        for i in range(len(poss)):\n",
    "            docid = news_index[poss[i]]\n",
    "            imp['docs'].append(docid)\n",
    "            imp['labels'].append(1)\n",
    "        for i in range(len(negs)):\n",
    "            docid = news_index[negs[i]]\n",
    "            imp['docs'].append(docid)\n",
    "            imp['labels'].append(0)\n",
    "        Impressions.append(imp)\n",
    "        \n",
    "    userid = np.array(userid,dtype='int32')\n",
    "    \n",
    "    return Impressions, userid,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:49:11.154804Z",
     "start_time": "2022-04-21T00:49:08.627030Z"
    }
   },
   "outputs": [],
   "source": [
    "test_impressions, test_userids = get_test_input(test_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:53:15.161088Z",
     "start_time": "2022-04-21T00:53:15.133857Z"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    " \n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ',\n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK',\n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV',\n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    " \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    " \n",
    "    def call(self, x):\n",
    "        #如果只传入Q_seq,K_seq,V_seq，那么就不做Mask\n",
    "        #如果同时传入Q_seq,K_seq,V_seq,Q_len,V_len，那么对多余部分做Mask\n",
    "        if len(x) == 3:\n",
    "            Q_seq,K_seq,V_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        #对Q、K、V做线性变换\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)\n",
    "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
    "        #计算内积，然后mask，然后softmax\n",
    "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = K.softmax(A)\n",
    "        #输出并mask\n",
    "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n",
    "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n",
    "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:53:16.458351Z",
     "start_time": "2022-04-21T00:53:16.454133Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH=30\n",
    "MAX_SENTS=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:53:16.922074Z",
     "start_time": "2022-04-21T00:53:16.913963Z"
    }
   },
   "outputs": [],
   "source": [
    "def AttentivePooling(dim1,dim2):\n",
    "    vecs_input = Input(shape=(dim1,dim2),dtype='float32')\n",
    "    user_vecs =Dropout(0.2)(vecs_input)\n",
    "    user_att = Dense(200,activation='tanh')(user_vecs)\n",
    "    user_att = keras.layers.Flatten()(Dense(1)(user_att))\n",
    "    user_att = Activation('softmax')(user_att)\n",
    "    user_vec = keras.layers.Dot((1,1))([user_vecs,user_att])\n",
    "    model = Model(vecs_input,user_vec)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:53:18.550399Z",
     "start_time": "2022-04-21T00:53:18.541586Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_doc_encoder(length,word_dict,title_word_embedding_matrix):\n",
    "\n",
    "    sentence_input = Input(shape=(length,), dtype='int32')\n",
    "    title_word_embedding_layer = Embedding(len(word_dict)+1, 300, weights=[title_word_embedding_matrix],trainable=True)\n",
    "    word_vecs = title_word_embedding_layer(sentence_input)\n",
    "    droped_vecs = Dropout(0.2)(word_vecs)\n",
    "    #word_rep = Conv1D(400,kernel_size=3,activation='relu')(droped_vecs)\n",
    "    word_rep = Attention(20,20)([droped_vecs]*3)\n",
    "    droped_rep = Dropout(0.2)(word_rep)\n",
    "    title_vec = AttentivePooling(length,400)(droped_rep)\n",
    "    sentEncodert = Model(sentence_input, title_vec)\n",
    "    return sentEncodert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:54:48.367100Z",
     "start_time": "2022-04-21T00:54:48.253042Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class get_hir_train_generator(Sequence):\n",
    "    def __init__(self,news_info, clicked_news,user_id, news_id, label, batch_size):\n",
    "        self.news_info = news_info\n",
    "        \n",
    "        self.clicked_news = clicked_news\n",
    "\n",
    "        self.user_id = user_id\n",
    "        self.doc_id = news_id\n",
    "        self.label = label\n",
    "        \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.ImpNum = self.user_id.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.ImpNum / float(self.batch_size)))\n",
    "    \n",
    "    def __get_news(self,docids):\n",
    "        news_info = self.news_info[docids]\n",
    "        \n",
    "        return news_info\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx*self.batch_size\n",
    "        ed = (idx+1)*self.batch_size\n",
    "        if ed> self.ImpNum:\n",
    "            ed = self.ImpNum\n",
    "            \n",
    "        label = self.label[start:ed]\n",
    "\n",
    "        doc_ids = self.doc_id[start:ed]\n",
    "        info = self.__get_news(doc_ids)\n",
    "        \n",
    "        user_ids = self.user_id[start:ed]\n",
    "        clicked_ids = self.clicked_news[user_ids]\n",
    "        user_info = self.__get_news(clicked_ids)\n",
    "\n",
    "\n",
    "        return ([info,user_info],[label])\n",
    "    \n",
    "    \n",
    "class get_hir_user_generator(Sequence):\n",
    "    def __init__(self,news_info, clicked_news,batch_size):\n",
    "        self.news_info = news_info\n",
    "        \n",
    "        self.clicked_news = clicked_news\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.ImpNum = self.clicked_news.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.ImpNum / float(self.batch_size)))\n",
    "\n",
    "    \n",
    "    def __get_news(self,docids):\n",
    "        news_info = self.news_info[docids]\n",
    "\n",
    "        return news_info\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = idx*self.batch_size\n",
    "        ed = (idx+1)*self.batch_size\n",
    "        if ed> self.ImpNum:\n",
    "            ed = self.ImpNum\n",
    "            \n",
    "        clicked_ids = self.clicked_news[start:ed]\n",
    "        user_info = self.__get_news(clicked_ids)\n",
    "\n",
    "        return user_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:54:54.444416Z",
     "start_time": "2022-04-21T00:54:54.430313Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(user_scorings,news_scorings,Impressions):\n",
    "    AUC = []\n",
    "    MRR = []\n",
    "    nDCG5 = []\n",
    "    nDCG10 =[]\n",
    "    for i in range(len(Impressions)):\n",
    "        docids = Impressions[i]['docs']\n",
    "        labels = Impressions[i]['labels']\n",
    "        mean = np.mean(labels)\n",
    "\n",
    "        uv = user_scorings[i]\n",
    "        \n",
    "        docids = np.array(docids,dtype='int32')\n",
    "        nv = news_scorings[docids]\n",
    "        score = np.dot(nv,uv)\n",
    "        auc = roc_auc_score(labels,score)\n",
    "        mrr = mrr_score(labels,score)\n",
    "        ndcg5 = ndcg_score(labels,score,k=5)\n",
    "        ndcg10 = ndcg_score(labels,score,k=10)\n",
    "    \n",
    "        AUC.append(auc)\n",
    "        MRR.append(mrr)\n",
    "        nDCG5.append(ndcg5)\n",
    "        nDCG10.append(ndcg10)\n",
    "\n",
    "    AUC = np.array(AUC)\n",
    "    MRR = np.array(MRR)\n",
    "    nDCG5 = np.array(nDCG5)\n",
    "    nDCG10 = np.array(nDCG10)\n",
    "    \n",
    "    AUC = AUC.mean()\n",
    "    MRR = MRR.mean()\n",
    "    nDCG5 = nDCG5.mean()\n",
    "    nDCG10 = nDCG10.mean()\n",
    "    \n",
    "    return AUC, MRR, nDCG5, nDCG10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:55:01.505772Z",
     "start_time": "2022-04-21T00:55:01.490142Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_doc_encoder(mode):\n",
    "\n",
    "    length = 30\n",
    "    word_embedding_matrix = title_word_embedding_matrix\n",
    "    sentence_input = Input(shape=(length,),dtype='int32')\n",
    "    \n",
    "    word_embedding_layer = Embedding(word_embedding_matrix.shape[0], 300, weights=[word_embedding_matrix],trainable=True)\n",
    "    word_vecs = word_embedding_layer(sentence_input)\n",
    "    droped_vecs = Dropout(0.2)(word_vecs)\n",
    "    \n",
    "    if mode in ['NRMS','NAML']:\n",
    "        word_rep = Attention(20,20)([droped_vecs]*3)\n",
    "        droped_rep = Dropout(0.2)(word_rep)\n",
    "    elif mode == 'NRMS-ReLU':\n",
    "        word_rep = Attention(20,20)([droped_vecs]*3)\n",
    "        droped_rep = Dropout(0.2)(word_rep)\n",
    "    elif mode == 'FastFormer':\n",
    "        word_rep = Fastformer(20,20)([droped_vecs]*3)\n",
    "        droped_rep = Dropout(0.2)(word_rep)\n",
    "    elif mode in ['NAML-woRelu']:\n",
    "        word_rep = Conv1D(400,3,padding='same')(droped_vecs)\n",
    "        droped_rep = Dropout(0.2)(word_rep)\n",
    "    elif mode in ['NAML','LSTUR']:\n",
    "        word_rep = Conv1D(400,3,padding='same',activation='relu')(droped_vecs)\n",
    "        droped_rep = Dropout(0.2)(word_rep)\n",
    "    elif mode in ['NAML-Dense']:\n",
    "        word_rep = Conv1D(400,3,padding='same',activation='relu')(droped_vecs)\n",
    "        word_rep = Dense(400)(word_rep)\n",
    "        droped_rep = Dropout(0.2)(word_rep)      \n",
    "        \n",
    "    title_vec = AttentivePooling(length,400)(droped_rep)\n",
    "\n",
    "    sentEncodert = Model(sentence_input, title_vec)\n",
    "    return sentEncodert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:55:05.453485Z",
     "start_time": "2022-04-21T00:55:05.442915Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_user_encoder(mode):\n",
    "    user_vecs_input = Input(shape=(50,400))    \n",
    "    click_mask_input = Input(shape=(50,))\n",
    "    \n",
    "    user_vecs = Dropout(0.2)(user_vecs_input)\n",
    "\n",
    "    if mode=='NRMS':\n",
    "        user_vecs = Attention(20,20)([user_vecs]*3)\n",
    "        user_vec = AttentivePooling(50,400)(user_vecs)\n",
    "    elif mode in ['LSTUR','GRU']:\n",
    "        user_vec = GRU(400)(user_vecs)\n",
    "    elif mode in ['NAML']:\n",
    "        #user_vecs = Dense(400)(user_vecs)\n",
    "        user_vec = AttentivePooling(50,400)(user_vecs)\n",
    "    elif mode in ['NAML-Dense']:\n",
    "        user_vec = AttentivePooling(50,400)(user_vecs)\n",
    "        user_vec = Dense(400)(user_vec)\n",
    "        \n",
    "    return Model(user_vecs_input,user_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:55:06.862750Z",
     "start_time": "2022-04-21T00:55:06.850525Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_model(mode):\n",
    "    \n",
    "    MAX_LENGTH = 30  \n",
    "    news_encoder = get_doc_encoder(mode,)\n",
    "\n",
    "    user_encoder = get_user_encoder(mode)\n",
    "    \n",
    "    clicked_title_input = Input(shape=(50,MAX_LENGTH,), dtype='int32')\n",
    "\n",
    "    title_inputs = Input(shape=(1+npratio,MAX_LENGTH,),dtype='int32') \n",
    "\n",
    "    clicked_title_vecs = TimeDistributed(news_encoder)(clicked_title_input)\n",
    "    news_vecs = TimeDistributed(news_encoder)(title_inputs)\n",
    "    \n",
    "    news_vecs = Dropout(0.2)(news_vecs)\n",
    "    clicked_title_vecs = Dropout(0.2)(clicked_title_vecs)\n",
    "\n",
    "    user_vec = user_encoder(clicked_title_vecs)\n",
    "    \n",
    "\n",
    "    scores = keras.layers.Dot(axes=-1)([news_vecs,user_vec])\n",
    "\n",
    "    \n",
    "    \n",
    "    logits = keras.layers.Activation(keras.activations.softmax,name = 'recommend')(scores)     \n",
    "\n",
    "    model = Model([title_inputs,clicked_title_input,],logits) # max prob_click_positive\n",
    "    model.compile(loss=['categorical_crossentropy'],\n",
    "                  optimizer=Adam(lr=0.0001,),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    \n",
    "    return model,news_encoder,user_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T00:55:15.531544Z",
     "start_time": "2022-04-21T00:55:15.526576Z"
    }
   },
   "outputs": [],
   "source": [
    "train_generator = get_hir_train_generator(news_title,train_user['click'],train_user_id,train_sess,train_label,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T01:48:43.709301Z",
     "start_time": "2022-04-21T01:48:43.511504Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"attention_2\" (type Attention).\n\nin user code:\n\n    File \"/tmp/ipykernel_1048174/2751447696.py\", line 57, in call  *\n        A = K.permute_dimensions(A, (0,3,2,1))\n    File \"/home/gene/anaconda3/envs/torch/lib/python3.8/site-packages/keras/backend.py\", line 3288, in permute_dimensions\n        return tf.compat.v1.transpose(x, perm=pattern)\n\n    ValueError: Dimension must be 5 but is 4 for '{{node attention_2/transpose_7}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](attention_2/truediv, attention_2/transpose_7/perm)' with input shapes: [?,20,30,20,30], [4].\n\n\nCall arguments received:\n  • x=['tf.Tensor(shape=(None, 30, 300), dtype=float32)', 'tf.Tensor(shape=(None, 30, 300), dtype=float32)', 'tf.Tensor(shape=(None, 30, 300), dtype=float32)']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1048174/503074103.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'NAML'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnews_encoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mRes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1048174/1240481231.py\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mMAX_LENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnews_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_doc_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0muser_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_user_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1048174/465687863.py\u001b[0m in \u001b[0;36mget_doc_encoder\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'NRMS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NAML'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mword_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdroped_vecs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mdroped_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_rep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NRMS-ReLU'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"attention_2\" (type Attention).\n\nin user code:\n\n    File \"/tmp/ipykernel_1048174/2751447696.py\", line 57, in call  *\n        A = K.permute_dimensions(A, (0,3,2,1))\n    File \"/home/gene/anaconda3/envs/torch/lib/python3.8/site-packages/keras/backend.py\", line 3288, in permute_dimensions\n        return tf.compat.v1.transpose(x, perm=pattern)\n\n    ValueError: Dimension must be 5 but is 4 for '{{node attention_2/transpose_7}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](attention_2/truediv, attention_2/transpose_7/perm)' with input shapes: [?,20,30,20,30], [4].\n\n\nCall arguments received:\n  • x=['tf.Tensor(shape=(None, 30, 300), dtype=float32)', 'tf.Tensor(shape=(None, 30, 300), dtype=float32)', 'tf.Tensor(shape=(None, 30, 300), dtype=float32)']"
     ]
    }
   ],
   "source": [
    "mode = 'NAML'\n",
    "model,news_encoder,user_encoder = create_model(mode)\n",
    "Res = []\n",
    "model.fit_generator(train_generator,epochs=1,verbose=1)\n",
    "for i in range(3):\n",
    "    model.fit_generator(train_generator,epochs=1,verbose=1)\n",
    "    news_scoring = news_encoder.predict(news_title,verbose=True)\n",
    "    test_user_generator = get_hir_user_generator(news_scoring,test_user['click'],32)\n",
    "    user_scoring = user_encoder.predict_generator(test_user_generator,verbose=True)\n",
    "    g = evaluate(user_scoring,news_scoring,test_impressions)\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6776793480631789,\n",
       " 0.3298717043234166,\n",
       " 0.3649363442750286,\n",
       " 0.4291449716730456)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T01:47:52.624677Z",
     "start_time": "2022-04-21T01:47:52.614632Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 09:47:52.618079: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
