{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T09:07:10.266292Z",
     "start_time": "2022-04-30T09:07:10.252159Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T09:07:11.840026Z",
     "start_time": "2022-04-30T09:07:11.008366Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from pytorchtools import EarlyStopping\n",
    "assert(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T02:10:17.729812Z",
     "start_time": "2022-04-30T02:10:17.723323Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "def logger(content):\n",
    "    logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "    log_format = '[%(asctime)s] %(message)s'\n",
    "    date_format = '%Y-%m-%d %H:%M:%S'\n",
    "    logging.basicConfig(level = logging.DEBUG, format = log_format, datefmt = date_format)\n",
    "    logging.info(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # merge news to one document\n",
    "# news_set = set()\n",
    "# news = []\n",
    "# with open('/data/Recommend/MIND/MINDsmall_train/news.tsv', 'r') as f:\n",
    "#     for line in f:\n",
    "#         data = line.split('\\t')\n",
    "#         news_id = data[0]\n",
    "#         if news_id not in news_set:\n",
    "#             news.append(line)\n",
    "#             news_set.add(news_id)\n",
    "# with open('/data/Recommend/MIND/MINDsmall_dev/news.tsv') as f:\n",
    "#     for line in f:\n",
    "#         data = line.split('\\t')\n",
    "#         news_id = data[0]\n",
    "#         if news_id not in news_set:\n",
    "#             news.append(line)\n",
    "#             news_set.add(news_id)\n",
    "# # with open('/data/Recommend/MIND/MINDlarge_test/news.tsv') as f:\n",
    "# #     for line in f:\n",
    "# #         data = line.split('\\t')\n",
    "# #         news_id = data[0]\n",
    "# #         if news_id not in news_set:\n",
    "# #             news.append(line)\n",
    "# #             news_set.add(news_id)\n",
    "\n",
    "# # with open('/data/Recommend/MIND/small_news.tsv', 'w') as f:\n",
    "# #     f.writelines(news)\n",
    "\n",
    "# print(len(news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T09:07:15.947145Z",
     "start_time": "2022-04-30T09:07:15.928817Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_news(path):\n",
    "    news_dict = {} # index -> news\n",
    "    news_list = [] # index -> news\n",
    "    newsid_dict = {} # newsid -> index\n",
    "    word_dict = {'<PAD>': 0, '<OOV>': 1}\n",
    "    cate_dict = {'<PAD>': 0, '<OOV>': 1}\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            news_id, category, subcategory, title, abstract, \\\n",
    "                url, title_entities, abstract_entities = line.strip().split('\\t')\n",
    "            title = title.lower().replace('.', '').replace(',', '').replace(';', '').replace(':', '').replace('\\'', '').replace('\"', '').replace('?', '').replace('!', '').replace('(', '').replace(')', '').split(' ')\n",
    "            abstract = abstract.lower().replace('.', '').replace(',', '').replace(';', '').replace(':', '').replace('\\'', '').replace('\"', '').replace('?', '').replace('!', '').replace('(', '').replace(')', '').split(' ')\n",
    "            for word in title + abstract:\n",
    "                if word not in word_dict:\n",
    "                    word_dict[word] = len(word_dict)\n",
    "            if category not in cate_dict:\n",
    "                cate_dict[category] = len(cate_dict)\n",
    "            if subcategory not in cate_dict:\n",
    "                cate_dict[subcategory] = len(cate_dict)\n",
    "            if news_id not in newsid_dict:\n",
    "                newsid_dict[news_id] = len(newsid_dict)\n",
    "                news_list.append([category, subcategory, title, abstract])\n",
    "    print(len(news_list))\n",
    "    return news_list, newsid_dict, word_dict, cate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T09:07:16.891086Z",
     "start_time": "2022-04-30T09:07:16.878463Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "max_title = 30\n",
    "max_body = 100\n",
    "def map_news_input(news_list, word_dict, cate_dict):\n",
    "    n_news = len(news_list)\n",
    "    titles = np.zeros((n_news, max_title), dtype = 'int32')\n",
    "    bodys = np.zeros((n_news, max_body), dtype = 'int32')\n",
    "    cates = np.zeros((n_news,1), dtype = 'int32')\n",
    "    subcates = np.zeros((n_news,1), dtype = 'int32')\n",
    "    for i in range(n_news):\n",
    "        category, subcategory, title, abstract = news_list[i]\n",
    "        titles[i, :len(title)] = [word_dict[word] for word in title[:max_title]]\n",
    "        bodys[i, :len(abstract)] = [word_dict[word] for word in abstract[:max_body]]\n",
    "        cates[i] = cate_dict[category]\n",
    "        subcates[i] = cate_dict[subcategory]\n",
    "    news_info = np.concatenate((titles, bodys, cates, subcates), axis = 1)\n",
    "    print(news_info.shape)\n",
    "    return news_info # index -> news_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T09:07:19.539743Z",
     "start_time": "2022-04-30T09:07:17.380827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65238\n",
      "(65238, 132)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "news_list: original news\n",
    "news_info: mapped news(word ids)\n",
    "'''\n",
    "news_list, newsid_dict, word_dict, cate_dict = load_news('/data/Recommend/MIND/small_news.tsv')\n",
    "news_info = map_news_input(news_list, word_dict, cate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T09:07:20.666956Z",
     "start_time": "2022-04-30T09:07:20.655774Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_glove(word_to_ix, dim = 100):\n",
    "    if dim == 100:\n",
    "        path = '/data/pretrained/Glove/glove.6B.100d.txt'\n",
    "    elif dim == 300:\n",
    "        path = '/data/pretrained/Glove/glove.840B.300d.txt'\n",
    "    word_emb = []\n",
    "    word_emb = np.zeros((len(word_to_ix), dim), dtype = float)\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = line.strip().split(' ') # [word emb1 emb2 ... emb n]\n",
    "            word = data[0]\n",
    "            if word in word_to_ix:\n",
    "                word_emb[word_to_ix[word]] = [float(i) for i in data[1:]]\n",
    "    print(word_emb.shape)\n",
    "    return torch.tensor(word_emb, dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T09:08:00.137605Z",
     "start_time": "2022-04-30T09:07:21.647368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80416, 300)\n",
      "(282, 100)\n"
     ]
    }
   ],
   "source": [
    "word_emb = load_glove(word_dict, 300)\n",
    "cate_emb = load_glove(cate_dict, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T09:13:40.938782Z",
     "start_time": "2022-04-30T09:13:40.926463Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_train_impression(path, newsid_dict): # train&dev\n",
    "    logs = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            imp_id, user_id, time, history, impression = line.strip().split('\\t')\n",
    "            if history:\n",
    "                history = [newsid_dict[news_id] for news_id in history.split(' ')]\n",
    "            else:\n",
    "                history = []\n",
    "            positive = []\n",
    "            negative = []\n",
    "            for item in impression.split(' '):\n",
    "                news_id, num = item.split('-')\n",
    "                if num == '1':\n",
    "                    positive.append(newsid_dict[news_id])\n",
    "                else:\n",
    "                    negative.append(newsid_dict[news_id])\n",
    "            logs.append([history, positive, negative]) # indexs\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T09:13:42.150644Z",
     "start_time": "2022-04-30T09:13:42.142637Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "max_history = 50\n",
    "def map_user(logs): # index -> history, 用 index 代表 user_id, train&dev\n",
    "    n_user = len(logs)\n",
    "    user_hist = np.zeros((n_user, max_history), dtype = 'int32') # index -> history\n",
    "    for i in range(n_user):\n",
    "        history, positive, negative = logs[i]\n",
    "        n_hist = len(history)\n",
    "        if n_hist == 0:\n",
    "            continue\n",
    "        user_hist[i, -n_hist:] = history[-max_history:]\n",
    "    return user_hist         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T09:13:42.380055Z",
     "start_time": "2022-04-30T09:13:42.362238Z"
    },
    "code_folding": [
     1,
     7,
     28
    ]
   },
   "outputs": [],
   "source": [
    "neg_ratio = 4\n",
    "def neg_sample(negative):\n",
    "    if len(negative) < neg_ratio:\n",
    "        return random.sample(negative * (neg_ratio // len(negative) + 1), neg_ratio)\n",
    "    else:\n",
    "        return random.sample(negative, neg_ratio)\n",
    "\n",
    "def get_train_input(logs): # 和 map_user 使用同一个 log\n",
    "    all_pos = [] # 每个 sample 的 pos\n",
    "    all_neg = []\n",
    "    user_id = [] # 每个 sample 的 user，用 index 表示，和 map_user 的结果对应\n",
    "    for i in range(len(logs)):\n",
    "        history, positive, negative = logs[i]\n",
    "        for pos in positive:\n",
    "            all_pos.append(pos)\n",
    "            all_neg.append(neg_sample(negative))\n",
    "            user_id.append(i)\n",
    "    n_imps = len(all_pos)\n",
    "    imps = np.zeros((n_imps, 1 + neg_ratio), dtype = 'int32')\n",
    "    for i in range(len(all_pos)):\n",
    "        imps[i, 0] = all_pos[i]\n",
    "        imps[i, 1:] = all_neg[i]\n",
    "    user_id = np.array(user_id, dtype = 'int32')\n",
    "    labels = np.zeros((n_imps, 1 + neg_ratio), dtype = 'int32')\n",
    "    labels[:, 0] = 1\n",
    "    print(n_imps)\n",
    "    return imps, user_id, labels\n",
    "\n",
    "def get_dev_input(logs): # 和 map_user 使用同一个 log\n",
    "    imps = []\n",
    "    labels = []\n",
    "    user_id = np.zeros((len(logs)), dtype = 'int32') # 每个 sample 的 user index，和 map_user 的结果对应\n",
    "    for i in range(len(logs)):\n",
    "        history, positive, negative = logs[i]\n",
    "        imps.append(np.array(positive + negative, dtype = 'int32'))\n",
    "        labels.append([1] * len(positive) + [0] * len(negative))\n",
    "        user_id[i] = i\n",
    "    print(len(logs))\n",
    "    return imps, user_id, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T02:38:30.236801Z",
     "start_time": "2022-04-30T02:38:30.231735Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # merge entity embedding to one document\n",
    "# ent_set = set()\n",
    "# ents = []\n",
    "# with open('/data/Recommend/MIND/MINDsmall_train/entity_embedding.vec', 'r') as f:\n",
    "#     for line in f:\n",
    "#         ent_id = line.split('\\t')[0]\n",
    "#         if ent_id not in ent_set:\n",
    "#             ents.append(line)\n",
    "#             ent_set.add(ent_id)\n",
    "# with open('/data/Recommend/MIND/MINDsmall_dev/entity_embedding.vec') as f:\n",
    "#     for line in f:\n",
    "#         ent_id = line.split('\\t')[0]\n",
    "#         if ent_id not in ent_set:\n",
    "#             ents.append(line)\n",
    "#             ent_set.add(ent_id)\n",
    "# # with open('/data/Recommend/MIND/MINDlarge_test/entity_embedding.vec') as f:\n",
    "# #     for line in f:\n",
    "# #         ent_id = line.split('\\t')[0]\n",
    "# #         if ent_id not in ent_set:\n",
    "# #             ents.append(line)\n",
    "# #             ent_set.add(ent_id)\n",
    "\n",
    "# with open('/data/Recommend/MIND/small_entity_embedding.vec', 'w') as f:\n",
    "#     f.writelines(ents)\n",
    "\n",
    "# print(len(ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T09:13:45.954942Z",
     "start_time": "2022-04-30T09:13:45.944953Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_ent_emb(path):\n",
    "    ent_emb = []\n",
    "    ent_dict = {'<PAD>': 0, '<OOV>': 1}\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = line.strip().split('\\t')\n",
    "            ent_id = data[0]\n",
    "            ent_dict[ent_id] = len(ent_dict)\n",
    "            ent_emb.append([float(i) for i in data[1:]])\n",
    "    ent_emb.insert(0, [0.] * len(ent_emb[0]))\n",
    "    ent_emb.insert(0, [0.] * len(ent_emb[0]))\n",
    "    ent_emb = torch.tensor(ent_emb, dtype = torch.float)\n",
    "    print(ent_emb.shape)\n",
    "    return ent_emb, ent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T11:07:08.205548Z",
     "start_time": "2022-04-30T11:07:08.194407Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "max_ents = 5\n",
    "def load_news_ent(path, ent_dict):\n",
    "    n_news = len(news_list)\n",
    "    news_ents = np.zeros((n_news, max_ents), dtype = 'int32')\n",
    "    i = 0\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            data = line.strip().split('\\t')\n",
    "            ents = [ent['WikidataId'] for ent in json.loads(data[6])] + [ent['WikidataId'] for ent in json.loads(data[7])]\n",
    "            news_ents[i, :len(ents)] = [ent_dict[ent] if ent in ent_dict else ent_dict['<OOV>'] for ent in ents[:max_ents]]\n",
    "            i += 1\n",
    "    print(len(news_ents))\n",
    "    return news_ents # index -> ent_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T11:07:09.474071Z",
     "start_time": "2022-04-30T11:07:08.525787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65238\n"
     ]
    }
   ],
   "source": [
    "ent_emb, ent_dict = load_ent_emb('/data/Recommend/MIND/small_entity_embedding.vec')\n",
    "news_ents = load_news_ent('/data/Recommend/MIND/small_news.tsv', ent_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T09:13:50.961507Z",
     "start_time": "2022-04-30T09:13:50.937845Z"
    },
    "code_folding": [
     32
    ]
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, imp_datas, imp_users, imp_labels, news_info, user_clicks, batch_size, news_ents = None):\n",
    "        self.imp_datas = imp_datas # (n_imps, 1 + k)\n",
    "        self.imp_users = imp_users\n",
    "        self.imp_labels = imp_labels\n",
    "        self.news = news_info\n",
    "        self.user_clicks = user_clicks\n",
    "        self.batch_size = batch_size\n",
    "        self.news_ents = news_ents\n",
    "        \n",
    "        self.n_data = imp_datas.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.n_data / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.batch_size\n",
    "        end = min((idx + 1) * self.batch_size, self.n_data)\n",
    "        \n",
    "        data_id = self.imp_datas[start: end] # (n_batch, 1 + k)\n",
    "        data_news = self.news[data_id] # (n_batch, 1 + k, news_len)\n",
    "        user_id = self.imp_users[start: end] # (n_batch)\n",
    "        user_news_id = self.user_clicks[user_id] # (n_batch, n_hist)\n",
    "        user_news = self.news[user_news_id] # (n_batch, n_hist, news_len)\n",
    "        labels = self.imp_labels[start: end] # (n_batch, 1 + k)\n",
    "        \n",
    "        if self.news_ents is not None:\n",
    "            samp_ents = self.news_ents[data_id]\n",
    "            user_ents = self.news_ents[user_news_id]\n",
    "            return data_news, user_news, labels, samp_ents, user_ents\n",
    "        \n",
    "        return data_news, user_news, labels\n",
    "    \n",
    "class DevDataset(Dataset): # data 和 label 是 list，每条数据不同长度\n",
    "    def __init__(self, imp_datas, imp_users, imp_labels, news_info, user_clicks, batch_size):\n",
    "        self.imp_datas = imp_datas # [imp1, imp2, ..., impn]\n",
    "        self.imp_users = imp_users # (n_imps)\n",
    "        self.imp_labels = imp_labels\n",
    "        self.news = news_info\n",
    "        self.user_clicks = user_clicks\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.n_data = len(imp_datas)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.n_data / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.batch_size\n",
    "        end = min((idx + 1) * self.batch_size, self.n_data)\n",
    "        \n",
    "        data_ids = []\n",
    "        data_news = [] # [(n_imp, news_len)]\n",
    "        labels = [] # [(n_imp)]\n",
    "        for i in range(start, end):\n",
    "            data_id = self.imp_datas[i] # (n_imp)\n",
    "            data_ids.append(data_id)\n",
    "            data_news.append(self.news[data_id]) # (n_imp, news_len)\n",
    "            labels.append(self.imp_labels[i]) # (n_imp)\n",
    "        user_id = self.imp_users[start: end] # (n_batch)\n",
    "        user_news_id = self.user_clicks[user_id] # (n_batch, n_hist)\n",
    "        user_news = self.news[user_news_id] # (n_batch, n_hist, news_len)\n",
    "        \n",
    "        #return data_news, user_news, labels\n",
    "        return data_ids, user_news_id, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T09:14:03.003880Z",
     "start_time": "2022-04-30T09:13:52.841772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236344\n",
      "73152\n",
      "111383\n"
     ]
    }
   ],
   "source": [
    "n_batch = 16\n",
    "train_logs = load_train_impression('/data/Recommend/MIND/MINDsmall_train/behaviors.tsv', newsid_dict)\n",
    "train_user_hist = map_user(train_logs)\n",
    "train_datas, train_users, train_labels = get_train_input(train_logs)\n",
    "train_dataset = TrainDataset(train_datas, train_users, train_labels, news_info, train_user_hist, n_batch)\n",
    "\n",
    "dev_logs = load_train_impression('/data/Recommend/MIND/MINDsmall_dev/behaviors.tsv', newsid_dict)\n",
    "dev_user_hist = map_user(dev_logs)\n",
    "dev_datas, dev_users, dev_labels = get_dev_input(dev_logs)\n",
    "dev_dataset = DevDataset(dev_datas, dev_users, dev_labels, news_info, dev_user_hist, 64)\n",
    "\n",
    "valid_datas, valid_users, valid_labels = get_train_input(dev_logs) # 用 train 的方法构造 dev_set\n",
    "valid_dataset = TrainDataset(valid_datas, valid_users, valid_labels, news_info, dev_user_hist, n_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T09:14:03.771545Z",
     "start_time": "2022-04-30T09:14:03.760826Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, emb_dim, query_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(emb_dim, query_dim)\n",
    "        self.fc2 = nn.Linear(query_dim, 1)\n",
    "        \n",
    "    def forward(self, x, mask = None):\n",
    "        '''\n",
    "        (n_batch, n_seq, emb_dim) -> (n_batch, emb_dim)\n",
    "        a = q^T tanh(V * k + v)\n",
    "        alpha = softmax(a)\n",
    "        '''\n",
    "        a = self.fc2(torch.tanh(self.fc1(x))) # (n_batch, n_seq, 1)\n",
    "        if mask is not None:\n",
    "            a = a.masked_fill(mask.unsqueeze(-1) == 0, -1e9)\n",
    "        alpha = F.softmax(a, dim = -2) # (n_batch, n_seq, 1)\n",
    "        r = torch.matmul(alpha.transpose(-2, -1), x).squeeze(-2) # (n_batch, emb_dim)\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T09:41:27.255557Z",
     "start_time": "2022-04-26T09:41:27.246307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 1]) torch.Size([3, 2, 5])\n",
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "ap = AttentionPooling(5, 10)\n",
    "a = torch.ones((3, 2, 5))\n",
    "print(ap(a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T09:14:06.557298Z",
     "start_time": "2022-04-30T09:14:06.541218Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, word_embedding, word_emb_dim, \n",
    "                 filter_num, window_size, query_dim, dropout, use_relu = False\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.use_relu = use_relu\n",
    "        self.word_embedding = word_embedding\n",
    "        self.cnn = nn.Conv1d(word_emb_dim, filter_num, window_size, padding = window_size // 2)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        self.attn = AttentionPooling(filter_num, query_dim)\n",
    "        \n",
    "    def forward(self, x, mask = None):\n",
    "        x_emb = self.word_embedding(x) # (n_batch, n_seq, emb_dim)\n",
    "        x_emb = self.drop1(x_emb)\n",
    "        x_rep = self.cnn(x_emb.transpose(2, 1)).transpose(2, 1) # (n_batch, n_seq, emb_dim)\n",
    "        if self.use_relu:\n",
    "            x_rep = F.relu(x_rep)\n",
    "        x_rep = self.drop2(x_rep)\n",
    "        x_rep = self.attn(x_rep, mask) # (n_batch, emb_dim)\n",
    "        return x_rep\n",
    "\n",
    "class CateEncoder(nn.Module):\n",
    "    def __init__(self, cate_embedding, cate_emb_dim, out_dim, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.cate_embedding = cate_embedding\n",
    "        self.fc = nn.Linear(cate_emb_dim, out_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_emb = self.cate_embedding(x) # (n_batch, emb_dim)\n",
    "        x_emb = self.drop(x_emb)\n",
    "        x_rep = self.fc(x_emb) # (n_batch, out_dim)\n",
    "        x_rep = F.relu(x_rep)\n",
    "        return x_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T09:39:41.093875Z",
     "start_time": "2022-04-30T09:39:41.071014Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class ConvNewsEncoder(nn.Module):\n",
    "    def __init__(self, word_emb, cate_emb, \n",
    "                 filter_num, window_size, query_dim, dropout, args\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        if 'use_relu' not in args:\n",
    "            args['use_relu'] = False\n",
    "        self.word_embedding = nn.Embedding.from_pretrained(word_emb)\n",
    "        self.cate_embedding = nn.Embedding.from_pretrained(cate_emb)\n",
    "        self.word_emb_dim = word_emb.shape[1]\n",
    "        self.cate_emb_dim = cate_emb.shape[1]\n",
    "        self.title_encoder = TextEncoder(self.word_embedding, self.word_emb_dim, \n",
    "                                 filter_num, window_size, query_dim, dropout, args['use_relu'])\n",
    "        if args['use_body']:\n",
    "            self.body_encoder = TextEncoder(self.word_embedding, self.word_emb_dim, \n",
    "                                     filter_num, window_size, query_dim, dropout, args['use_relu'])\n",
    "            self.attn = AttentionPooling(filter_num, query_dim)\n",
    "        if args['use_cate']:\n",
    "            self.cate_encoder = CateEncoder(self.cate_embedding, self.cate_emb_dim, filter_num, dropout)\n",
    "            self.subcate_encoder = CateEncoder(self.cate_embedding, self.cate_emb_dim, filter_num, dropout)\n",
    "            self.attn = AttentionPooling(filter_num, query_dim)\n",
    "    \n",
    "    def forward(self, news):\n",
    "        title, body, cate, subcate = news[:, :max_title], news[:, max_title: -2], news[:, -2], news[:, -1]\n",
    "        \n",
    "        r_t = self.title_encoder(title) # (n_news, emb_dim)\n",
    "        \n",
    "        if self.args['use_body'] and args['use_cate']:\n",
    "            r_b = self.body_encoder(body) # (n_news, emb_dim)\n",
    "            r_c = self.cate_encoder(cate) # (n_news, emb_dim)\n",
    "            r_sc = self.subcate_encoder(subcate) # (n_news, emb_dim)\n",
    "            r = torch.stack((r_t, r_b, r_c, r_sc), dim = 1) # (n_news, 4, emb_dim)\n",
    "            r = self.attn(r) # (n_news, n_filter)\n",
    "        elif self.args['use_body']:\n",
    "            r_b = self.body_encoder(body) # (n_news, emb_dim)\n",
    "            r = torch.stack((r_t, r_b), dim = 1) # (n_news, 4, emb_dim)\n",
    "            r = self.attn(r) # (n_news, n_filter)\n",
    "        elif self.args['use_cate']:\n",
    "            r_c = self.cate_encoder(cate) # (n_news, emb_dim)\n",
    "            r_sc = self.subcate_encoder(subcate) # (n_news, emb_dim)\n",
    "            r = torch.stack((r_t, r_c, r_sc), dim = 1) # (n_news, 4, emb_dim)\n",
    "            r = self.attn(r) # (n_news, n_filter)\n",
    "        else:\n",
    "            r = r_t\n",
    "        return r # (n_news, n_filter)\n",
    "\n",
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self, emb_dim, query_dim):\n",
    "        super().__init__()\n",
    "        self.attn = AttentionPooling(emb_dim, query_dim)\n",
    "    \n",
    "    def forward(self, h, mask = None): \n",
    "        u = self.attn(h, mask)\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T07:58:53.885805Z",
     "start_time": "2022-04-25T07:58:53.872942Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class NAML(nn.Module):\n",
    "    def __init__(self, word_emb, cate_emb, args):\n",
    "        super().__init__()\n",
    "        filter_num, window_size, query_dim, dropout = 400, 3, 200, 0.2\n",
    "        self.news_encoder = ConvNewsEncoder(word_emb, cate_emb, filter_num, window_size, query_dim, dropout, args)\n",
    "        self.user_encoder = UserEncoder(filter_num, query_dim)\n",
    "    \n",
    "    def forward(self, hist, samp):\n",
    "        n_batch, n_news, n_sequence = hist.shape\n",
    "        n_samp = samp.shape[1] # k + 1\n",
    "        \n",
    "        hist = hist.reshape(n_batch * n_news, n_sequence)\n",
    "        h = self.news_encoder(hist) # (n_batch*n_news, n_filter)\n",
    "        h = h.reshape(n_batch, n_news, -1)  # (n_batch, n_news, n_filter)\n",
    "        u = self.user_encoder(h) # (n_batch, n_filter)\n",
    "        \n",
    "        samp = samp.reshape(n_batch * n_samp, n_sequence)\n",
    "        r = self.news_encoder(samp) # (n_batch*(k+1), n_filter)\n",
    "        r = r.reshape(n_batch, n_samp, -1) # (n_batch, k + 1, n_filter)\n",
    "        \n",
    "        y = torch.bmm(r, u.unsqueeze(2)) # (n_batch, K + 1, 1)\n",
    "        return y.squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T12:40:30.895159Z",
     "start_time": "2022-04-30T12:40:30.880980Z"
    },
    "code_folding": [
     0,
     11
    ]
   },
   "outputs": [],
   "source": [
    "def encode_all_news(news_info, news_encoder):\n",
    "    n_news = len(news_info)\n",
    "    news_rep = []\n",
    "    n_batch = 32\n",
    "    for i in range((len(news_info) + n_batch - 1) // n_batch):\n",
    "        batch_news = torch.tensor(news_info[i * n_batch: (i + 1) * n_batch], dtype = torch.long, device = 'cuda')\n",
    "        batch_rep = news_encoder(batch_news).detach().cpu().numpy()\n",
    "        news_rep.append(batch_rep)\n",
    "    news_rep = np.concatenate(news_rep, axis = 0)\n",
    "    return news_rep # (n_news, n_title, n_emb)\n",
    "\n",
    "def encode_all_user(user_ids, user_hist, user_encoder, news_rep):\n",
    "    user_rep = []\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(dev_dataset):\n",
    "            if len(batch[0]) == 0:\n",
    "                break\n",
    "            user_hist_rep = torch.tensor(news_rep[batch[1]], device = 'cuda') # (n_batch, n_hist)\n",
    "            user = model.user_encoder(user_hist_rep).detach().cpu().numpy() # (n_batch, emb_dim)\n",
    "            user_rep.append(user)\n",
    "    # user_rep = np.concatenate(user_rep, axis = 0)\n",
    "    return user_rep # [user_rep, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T12:40:34.681486Z",
     "start_time": "2022-04-30T12:40:34.669717Z"
    },
    "code_folding": [
     0,
     8,
     14
    ]
   },
   "outputs": [],
   "source": [
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    return actual / best\n",
    "\n",
    "def mrr_score(y_true, y_score):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "    return np.sum(rr_score) / np.sum(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-23T08:57:15.620445Z",
     "start_time": "2022-04-23T08:34:30.564085Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# train\n",
    "def train(train_dataset, valid_dataset = None, epochs = 10):\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-4)\n",
    "    entrophy = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        train_losses = []\n",
    "        model.train()\n",
    "        for _, batch in enumerate(train_dataset):\n",
    "            if batch[0][0].shape[0] == 0:\n",
    "                break\n",
    "            sample = torch.tensor(batch[0], dtype = torch.long, device = device)\n",
    "            history = torch.tensor(batch[1], dtype = torch.long, device = device)\n",
    "            correct = torch.argmax(torch.tensor(batch[2], dtype = torch.long, device = device), dim = 1)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(history, sample)\n",
    "            loss = entrophy(output, correct)\n",
    "            train_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        logger('[epoch {:d}] train_loss: {:.4f}'.format(epoch + 1, np.average(train_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T07:18:11.929062Z",
     "start_time": "2022-04-30T07:18:11.908884Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# train with valid\n",
    "def train(model, train_dataset, valid_dataset = None, epochs = 4):\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-4)\n",
    "    entrophy = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        model.train()\n",
    "        for _, batch in tqdm(enumerate(train_dataset)):\n",
    "            if batch[0].shape[0] == 0:\n",
    "                break\n",
    "            # torch.Size([16, 5, 30]) torch.Size([16, 50, 30]) torch.Size([16])\n",
    "            sample = torch.tensor(batch[0], dtype = torch.long, device = device)\n",
    "            history = torch.tensor(batch[1], dtype = torch.long, device = device)\n",
    "            correct = torch.argmax(torch.tensor(batch[2], dtype = torch.long, device = device), dim = 1)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(history, sample)\n",
    "            loss = entrophy(output, correct)\n",
    "            train_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if valid_dataset is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for _, batch in enumerate(valid_dataset):\n",
    "                    if batch[0].shape[0] == 0:\n",
    "                        break\n",
    "                    sample = torch.tensor(batch[0], dtype = torch.long, device = device)\n",
    "                    history = torch.tensor(batch[1], dtype = torch.long, device = device)\n",
    "                    correct = torch.argmax(torch.tensor(batch[2], dtype = torch.long, device = device), dim = 1)\n",
    "                    output = model(sample, history)\n",
    "                    loss = entrophy(output, correct)\n",
    "                    valid_losses.append(loss.item())\n",
    "                print('[epoch {:d}] train_loss: {:.4f} valid_loss: {:.4f}'.format(epoch + 1, np.average(train_losses), np.average(valid_losses)))\n",
    "        else:\n",
    "            print('[epoch {:d}] train_loss: {:.4f}'.format(epoch + 1, np.average(train_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T09:14:12.991165Z",
     "start_time": "2022-04-30T09:14:12.977064Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dev_dataset, news_info, dev_users, dev_user_hist):\n",
    "    news_rep = encode_all_news(news_info, model.news_encoder) # (65238, 400)\n",
    "    user_rep = encode_all_user(dev_users, dev_user_hist, model.user_encoder, news_rep)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        auc_scores = []\n",
    "        mrr_scores = []\n",
    "        ndcg5_scores = []\n",
    "        ndcg10_scores = []\n",
    "        for i, batch in tqdm(enumerate(dev_dataset)):\n",
    "            if len(batch[0]) == 0:\n",
    "                break\n",
    "            user = user_rep[i]\n",
    "            for j in range(len(batch[0])):\n",
    "                sample = news_rep[batch[0][j]] # (n_imp, emb_dim)\n",
    "                positive = batch[2][j] # (1, n_imp)\n",
    "\n",
    "                score = np.matmul(sample, user[j]) # (1, n_imp)\n",
    "                predict = np.exp(score) / np.sum(np.exp(score))\n",
    "\n",
    "                auc_scores.append(roc_auc_score(positive, predict))\n",
    "                mrr_scores.append(mrr_score(positive, predict))\n",
    "                ndcg5_scores.append(ndcg_score(positive, predict, k = 5))\n",
    "                ndcg10_scores.append(ndcg_score(positive, predict, k = 10))\n",
    "    print('[Test] AUC: {:4f}, MRR: {:4f}, nDCG5:{:4f}, nDCG10: {:4f}'.format(\n",
    "        np.mean(auc_scores), np.mean(mrr_scores), np.mean(ndcg5_scores), np.mean(ndcg10_scores)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NRMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T02:39:14.821303Z",
     "start_time": "2022-04-30T02:39:14.805374Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def clones(module, n):\n",
    "#     return nn.ModuleList([deepcopy(module)] * n)\n",
    "\n",
    "# class SelfAttention(nn.Module): # word_level\n",
    "#     def __init__(self, emb_dim, query_dim, output_dim):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(emb_dim, query_dim)\n",
    "#         self.fc2 = nn.Linear(emb_dim, query_dim)\n",
    "#         self.fc3 = nn.Linear(emb_dim, output_dim)\n",
    "#     def forward(self, e): # (n_batch, n_seq, emb_dim)\n",
    "#         query = self.fc1(e) # (n_batch, n_seq, query_dim)\n",
    "#         key = self.fc2(e) # (n_batch, n_seq, query_dim)\n",
    "#         value = self.fc3(e) # (n_batch, n_seq, output_dim)\n",
    "        \n",
    "#         dot = torch.matmul(query, key.transpose(-1, -2)) # (n_batch, n_seq, n_seq)\n",
    "#         alpha = F.softmax(dot, dim = -1) # (n_batch, n_seq, n_seq)\n",
    "#         h = torch.matmul(alpha, value) # (n_batch, n_seq, output_dim)\n",
    "#         return h # (n_batch, n_seq, output_dim)\n",
    "\n",
    "# class MultiHeadSelfAttention(nn.Module):\n",
    "#     def __init__(self, h, emb_dim, query_dim, output_dim):\n",
    "#         super().__init__()\n",
    "#         self.h = h\n",
    "#         assert(output_dim % h == 0)\n",
    "#         self.layers = clones(SelfAttention(emb_dim, query_dim, output_dim // h), h)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         reps = [layer(x) for layer in self.layers]\n",
    "#         reps = torch.cat(reps, dim = -1)\n",
    "#         return reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T09:19:01.561985Z",
     "start_time": "2022-04-30T09:19:01.540548Z"
    },
    "code_folding": [
     0,
     15
    ]
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask=None):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.d_k)\n",
    "        scores = torch.exp(scores)\n",
    "        if attn_mask is not None:\n",
    "            scores = scores * attn_mask\n",
    "        attn = scores / (torch.sum(scores, dim=-1, keepdim=True)  + 1e-8)\n",
    "        \n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_k, d_v):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # 300\n",
    "        self.n_heads = n_heads # 20\n",
    "        self.d_k = d_k # 20\n",
    "        self.d_v = d_v # 20\n",
    "        \n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads) # 300, 400\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads) # 300, 400\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads) # 300, 400\n",
    "        \n",
    "        self._initialize_weights()\n",
    "                \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "                \n",
    "    def forward(self, Q, K, V, attn_mask=None):\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        \n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1,2)\n",
    "        \n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask.unsqueeze(1).expand(batch_size, max_len, max_len) \n",
    "            attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1) \n",
    "        \n",
    "        context, attn = ScaledDotProductAttention(self.d_k)(q_s, k_s, v_s, attn_mask) \n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v) \n",
    "        return context # (n_batch, n_seq, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T09:14:20.609987Z",
     "start_time": "2022-04-30T09:14:20.594431Z"
    },
    "code_folding": [
     0,
     21
    ]
   },
   "outputs": [],
   "source": [
    "class AttnNewsEncoder(nn.Module):\n",
    "    def __init__(self, word_emb, cate_emb, n_head, news_dim, query_dim):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.word_embedding = nn.Embedding.from_pretrained(word_emb)\n",
    "        emb_dim = word_emb.shape[1]\n",
    "        # self.self_attn = MultiHeadSelfAttention(n_head, emb_dim, query_dim, news_dim)\n",
    "        self.self_attn = MultiHeadSelfAttention(emb_dim, n_head, 16, 16)\n",
    "        self.addi_attn = AttentionPooling(news_dim, query_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, news):\n",
    "        title, body, cate, subcate = news[:, :max_title], news[:, max_title: -2], news[:, -2], news[:, -1]\n",
    "        \n",
    "        t_rep = self.word_embedding(title) # (n_batch, n_seq, emb_dim)\n",
    "        t_rep = self.dropout(t_rep)\n",
    "        t_rep = self.self_attn(t_rep, t_rep, t_rep) # (n_batch, n_seq, 256)\n",
    "        t_rep = self.addi_attn(t_rep) # (n_batch, 256)\n",
    "        \n",
    "        return t_rep # (n_news, 256)\n",
    "\n",
    "class AttnUserEncoder(nn.Module):\n",
    "    def __init__(self, n_head, news_dim, query_dim):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(news_dim, n_head, 16, 16)\n",
    "        self.addi_attn = AttentionPooling(news_dim, query_dim)\n",
    "    \n",
    "    def forward(self, h): # (n_batch, n_news, 256)\n",
    "        u = self.self_attn(h, h, h) # (n_batch, n_news, 256)\n",
    "        u = self.addi_attn(u) # (n_batch, 256)\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T07:15:17.459713Z",
     "start_time": "2022-04-30T07:15:17.447598Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class NRMS(nn.Module):\n",
    "    def __init__(self, word_emb, cate_emb, args):\n",
    "        super().__init__()\n",
    "        n_head, query_dim, news_dim = 16, 200, 256\n",
    "        self.news_encoder = AttnNewsEncoder(word_emb, cate_emb, n_head, news_dim, query_dim)\n",
    "        self.user_encoder = AttnUserEncoder(n_head, news_dim, query_dim)\n",
    "    \n",
    "    def forward(self, hist, samp):\n",
    "        n_batch, n_news, n_sequence = hist.shape\n",
    "        n_samp = samp.shape[1] # k + 1\n",
    "        \n",
    "        hist = hist.reshape(n_batch * n_news, n_sequence)\n",
    "        h = self.news_encoder(hist) # (n_batch*n_news, n_filter)\n",
    "        h = h.reshape(n_batch, n_news, -1)  # (n_batch, n_news, n_filter)\n",
    "        u = self.user_encoder(h) # (n_batch, n_filter)\n",
    "        \n",
    "        samp = samp.reshape(n_batch * n_samp, n_sequence)\n",
    "        r = self.news_encoder(samp) # (n_batch*(k+1), n_filter)\n",
    "        r = r.reshape(n_batch, n_samp, -1) # (n_batch, k + 1, n_filter)\n",
    "        \n",
    "        y = torch.bmm(r, u.unsqueeze(2)) # (n_batch, K + 1, 1)\n",
    "        return y.squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T07:24:38.485391Z",
     "start_time": "2022-04-30T07:18:18.855352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'NRMS'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2649bb05c96f442193958e7c94c6ef60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [60]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(args)\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m NRMS(word_emb, cate_emb, args)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m evaluate(model, dev_dataset, news_info, dev_users, dev_user_hist)\n",
      "Input \u001b[0;32mIn [59]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset, valid_dataset, epochs)\u001b[0m\n\u001b[1;32m     35\u001b[0m         logger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[epoch \u001b[39m\u001b[38;5;132;01m{:d}\u001b[39;00m\u001b[38;5;124m] train_loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m valid_loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, np\u001b[38;5;241m.\u001b[39maverage(train_losses), np\u001b[38;5;241m.\u001b[39maverage(valid_losses)))\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mlogger\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[epoch \u001b[39m\u001b[38;5;132;01m{:d}\u001b[39;00m\u001b[38;5;124m] train_loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, np\u001b[38;5;241m.\u001b[39maverage(train_losses)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "args = {'model': 'NRMS'}\n",
    "print(args)\n",
    "\n",
    "model = NRMS(word_emb, cate_emb, args).to(device)\n",
    "train(model, train_dataset)\n",
    "evaluate(model, dev_dataset, news_info, dev_users, dev_user_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T16:18:48.789450Z",
     "start_time": "2022-04-25T14:24:49.129654Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'NRMS'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-04-25 22:53:25] [epoch 1] train_loss: 1.4596\n",
      "[2022-04-25 23:21:58] [epoch 2] train_loss: 1.3942\n",
      "[2022-04-25 23:50:22] [epoch 3] train_loss: 1.3701\n",
      "[2022-04-26 00:18:39] [epoch 4] train_loss: 1.3574\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [302]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m NRMS(word_emb, cate_emb, args)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m train(model, train_dataset)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnews_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_user_hist\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [261]\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, dev_dataset, news_info, dev_users, dev_user_hist)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(model, dev_dataset, news_info, dev_users, dev_user_hist):\n\u001b[1;32m      2\u001b[0m     news_rep \u001b[38;5;241m=\u001b[39m encode_all_news(news_info, model\u001b[38;5;241m.\u001b[39mnews_encoder) \u001b[38;5;66;03m# (65238, 400)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     user_rep \u001b[38;5;241m=\u001b[39m \u001b[43mencode_all_user\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_user_hist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnews_rep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Input \u001b[0;32mIn [260]\u001b[0m, in \u001b[0;36mencode_all_user\u001b[0;34m(user_ids, user_hist, user_encoder, news_rep)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         user_hist_rep \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(news_rep[batch[\u001b[38;5;241m1\u001b[39m]], device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# (n_batch, n_hist)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m         user \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_hist_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;66;03m# (n_batch, emb_dim)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         user_rep\u001b[38;5;241m.\u001b[39mappend(user)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# user_rep = np.concatenate(user_rep, axis = 0)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "args = {'model': 'NRMS'}\n",
    "print(args)\n",
    "\n",
    "model = NRMS(word_emb, cate_emb, args).to(device)\n",
    "train(model, train_dataset)\n",
    "evaluate(model, dev_dataset, news_info, dev_users, dev_user_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T00:10:11.246779Z",
     "start_time": "2022-04-26T00:07:50.713930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f439ff51668c4332a42f348717b20dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] AUC: 0.642437, MRR: 0.294813, nDCG5:0.320318, nDCG10: 0.386850\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, dev_dataset, news_info, dev_users, dev_user_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T00:27:24.745092Z",
     "start_time": "2022-04-26T00:27:24.736129Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class GruUserEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, 1, batch_first = True)\n",
    "\n",
    "    def forward(self, h): # (n_batch, n_news, news_dim)\n",
    "        h0 = torch.randn((1, h.shape[0], self.hidden_size), device = 'cuda')\n",
    "        output, hn = self.gru(h, h0)\n",
    "        return hn.squeeze(0) # (n_batch, news_dim)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     47
    ]
   },
   "outputs": [],
   "source": [
    "class ConvNewsEncoder(nn.Module):\n",
    "    def __init__(self, word_emb, cate_emb, \n",
    "                 filter_num, window_size, query_dim, dropout, args\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        if 'use_relu' not in args:\n",
    "            args['use_relu'] = False\n",
    "        self.word_embedding = nn.Embedding.from_pretrained(word_emb)\n",
    "        self.cate_embedding = nn.Embedding.from_pretrained(cate_emb)\n",
    "        self.word_emb_dim = word_emb.shape[1]\n",
    "        self.cate_emb_dim = cate_emb.shape[1]\n",
    "        self.title_encoder = TextEncoder(self.word_embedding, self.word_emb_dim, \n",
    "                                 filter_num, window_size, query_dim, dropout, args['use_relu'])\n",
    "        if args['use_body']:\n",
    "            self.body_encoder = TextEncoder(self.word_embedding, self.word_emb_dim, \n",
    "                                     filter_num, window_size, query_dim, dropout, args['use_relu'])\n",
    "            self.attn = AttentionPooling(filter_num, query_dim)\n",
    "        if args['use_cate']:\n",
    "            self.cate_encoder = CateEncoder(self.cate_embedding, self.cate_emb_dim, filter_num, dropout)\n",
    "            self.subcate_encoder = CateEncoder(self.cate_embedding, self.cate_emb_dim, filter_num, dropout)\n",
    "            self.attn = AttentionPooling(filter_num, query_dim)\n",
    "    \n",
    "    def forward(self, news):\n",
    "        title, body, cate, subcate = news[:, :max_title], news[:, max_title: -2], news[:, -2], news[:, -1]\n",
    "        \n",
    "        r_t = self.title_encoder(title) # (n_news, emb_dim)\n",
    "        \n",
    "        if self.args['use_body'] and args['use_cate']:\n",
    "            r_b = self.body_encoder(body) # (n_news, emb_dim)\n",
    "            r_c = self.cate_encoder(cate) # (n_news, emb_dim)\n",
    "            r_sc = self.subcate_encoder(subcate) # (n_news, emb_dim)\n",
    "            r = torch.stack((r_t, r_b, r_c, r_sc), dim = 1) # (n_news, 4, emb_dim)\n",
    "            r = self.attn(r) # (n_news, n_filter)\n",
    "        elif self.args['use_body']:\n",
    "            r_b = self.body_encoder(body) # (n_news, emb_dim)\n",
    "            r = torch.stack((r_t, r_b), dim = 1) # (n_news, 4, emb_dim)\n",
    "            r = self.attn(r) # (n_news, n_filter)\n",
    "        elif self.args['use_cate']:\n",
    "            r_c = self.cate_encoder(cate) # (n_news, emb_dim)\n",
    "            r_sc = self.subcate_encoder(subcate) # (n_news, emb_dim)\n",
    "            r = torch.stack((r_t, r_c, r_sc), dim = 1) # (n_news, 4, emb_dim)\n",
    "            r = self.attn(r) # (n_news, n_filter)\n",
    "        else:\n",
    "            r = r_t\n",
    "        return r # (n_news, n_filter)\n",
    "\n",
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, args, word_emb, cate_emb, ent_emb = None):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.word_embedding = nn.Embedding.from_pretrained(word_emb)\n",
    "        self.cate_embedding = nn.Embedding.from_pretrained(cate_emb)\n",
    "        word_emb_dim = word_emb.shape[1]\n",
    "        cate_emb_dim = cate_emb.shape[1]\n",
    "        n_head, query_dim = 16, 200\n",
    "        if args['model'] == 'NAML':\n",
    "            self.cnn = nn.Conv1d(word_emb_dim, filter_num, window_size, padding = window_size // 2)\n",
    "            self.attn = AttentionPooling(filter_num, query_dim)\n",
    "            if args['use_cate']:\n",
    "                self.cate_fc1 = nn.Linear(cate_emb_dim, out_dim)\n",
    "                self.cate_fc2 = nn.Linear(cate_emb_dim, out_dim)\n",
    "        if args['model'] == 'NRMS':\n",
    "            self.self_attn = MultiHeadSelfAttention(word_emb_dim, n_head, 16, 16)\n",
    "            self.addi_attn = AttentionPooling(256, 200)\n",
    "        if args['model'] == 'LSTUR':\n",
    "            filter_num, window_size, query_dim, dropout = 300, 3, 200, 0.2\n",
    "            args['use_relu'] = True\n",
    "            self.news_encoder = ConvNewsEncoder(word_emb, cate_emb, filter_num, window_size, query_dim, dropout, args)\n",
    "            self.user_encoder = GruUserEncoder(filter_num, filter_num)\n",
    "        if args['model'] == 'CAUM':\n",
    "            filter_num, window_size, query_dim, dropout = 400, 3, 200, 0.2\n",
    "            args['use_relu'] = False\n",
    "            self.news_encoder = ConvNewsEncoder(word_emb, cate_emb, filter_num, window_size, query_dim, dropout, args)\n",
    "            self.user_encoder = UserEncoder(filter_num, query_dim)\n",
    "        self.word_embedding = nn.Embedding.from_pretrained(word_emb)\n",
    "        emb_dim = word_emb.shape[1]\n",
    "        self.self_attn = MultiHeadSelfAttention(emb_dim, n_head, 16, 16)\n",
    "        self.addi_attn = AttentionPooling(news_dim, query_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, news):\n",
    "        title, body, cate, subcate = news[:, :max_title], news[:, max_title: -2], news[:, -2], news[:, -1]\n",
    "        \n",
    "        t_rep = self.word_embedding(title) # (n_batch, n_seq, emb_dim)\n",
    "        t_rep = self.dropout(t_rep)\n",
    "        if args['model'] == 'NAML':\n",
    "            x_rep = self.cnn(x_emb.transpose(2, 1)).transpose(2, 1) # (n_batch, n_seq, emb_dim)\n",
    "            if self.use_relu:\n",
    "                x_rep = F.relu(x_rep)\n",
    "            x_rep = self.drop2(x_rep)\n",
    "            x_rep = self.attn(x_rep, mask) # (n_batch, emb_dim)\n",
    "            if args['use_cate']:\n",
    "                x_emb = self.cate_embedding(x) # (n_batch, emb_dim)\n",
    "                x_emb = self.drop(x_emb)\n",
    "                x_rep = self.fc(x_emb) # (n_batch, out_dim)\n",
    "                x_rep = F.relu(x_rep)\n",
    "        if args['model'] == 'NRMS':\n",
    "            t_rep = self.self_attn(t_rep, t_rep, t_rep) # (n_batch, n_seq, 256)\n",
    "            t_rep = self.addi_attn(t_rep) # (n_batch, 256)\n",
    "        \n",
    "        \n",
    "        return t_rep # (n_news, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T09:41:53.523032Z",
     "start_time": "2022-04-30T09:41:53.501754Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, word_emb, cate_emb, args):\n",
    "        super().__init__()\n",
    "        if args['model'] == 'NAML':\n",
    "            filter_num, window_size, query_dim, dropout = 400, 3, 200, 0.2\n",
    "            args['use_relu'] = False\n",
    "            self.news_encoder = ConvNewsEncoder(word_emb, cate_emb, filter_num, window_size, query_dim, dropout, args)\n",
    "            self.user_encoder = UserEncoder(filter_num, query_dim)\n",
    "        if args['model'] == 'NRMS':\n",
    "            n_head, query_dim, news_dim = 16, 200, 256\n",
    "            self.news_encoder = AttnNewsEncoder(word_emb, cate_emb, n_head, news_dim, query_dim)\n",
    "            self.user_encoder = AttnUserEncoder(n_head, news_dim, query_dim)\n",
    "        if args['model'] == 'LSTUR':\n",
    "            filter_num, window_size, query_dim, dropout = 300, 3, 200, 0.2\n",
    "            args['use_relu'] = True\n",
    "            self.news_encoder = ConvNewsEncoder(word_emb, cate_emb, filter_num, window_size, query_dim, dropout, args)\n",
    "            self.user_encoder = GruUserEncoder(filter_num, filter_num)\n",
    "        if args['model'] == 'CAUM':\n",
    "            filter_num, window_size, query_dim, dropout = 400, 3, 200, 0.2\n",
    "            args['use_relu'] = False\n",
    "            self.news_encoder = ConvNewsEncoder(word_emb, cate_emb, filter_num, window_size, query_dim, dropout, args)\n",
    "            self.user_encoder = UserEncoder(filter_num, query_dim)\n",
    "    \n",
    "    def forward(self, hist, samp, hist_ents = None, samp_ents = None):\n",
    "        n_batch, n_news, n_sequence = hist.shape\n",
    "        n_samp = samp.shape[1] # k + 1\n",
    "        \n",
    "        hist = hist.reshape(n_batch * n_news, n_sequence)\n",
    "        h = self.news_encoder(hist) # (n_batch*n_news, n_filter)\n",
    "        h = h.reshape(n_batch, n_news, -1)  # (n_batch, n_news, n_filter)\n",
    "        u = self.user_encoder(h) # (n_batch, n_filter)\n",
    "        \n",
    "        samp = samp.reshape(n_batch * n_samp, n_sequence)\n",
    "        r = self.news_encoder(samp) # (n_batch*(k+1), n_filter)\n",
    "        r = r.reshape(n_batch, n_samp, -1) # (n_batch, k + 1, n_filter)\n",
    "        y = torch.bmm(r, u.unsqueeze(2)) # (n_batch, K + 1, 1)\n",
    "        return y.squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T02:20:45.172427Z",
     "start_time": "2022-04-26T02:01:44.203884Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'LSTUR', 'use_body': False, 'use_cate': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-04-26 10:05:54] [epoch 1] train_loss: 1.4436\n",
      "[2022-04-26 10:10:11] [epoch 2] train_loss: 1.3969\n",
      "[2022-04-26 10:14:25] [epoch 3] train_loss: 1.3777\n",
      "[2022-04-26 10:18:39] [epoch 4] train_loss: 1.3632\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8fe251de84428485b87e6d719bf4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] AUC: 0.636074, MRR: 0.294337, nDCG5:0.321621, nDCG10: 0.386288\n"
     ]
    }
   ],
   "source": [
    "args = {'model': 'LSTUR', \n",
    "        'use_body': False,\n",
    "        'use_cate': False}\n",
    "print(args)\n",
    "\n",
    "model = Model(word_emb, cate_emb, args).to(device)\n",
    "train(model, train_dataset)\n",
    "evaluate(model, dev_dataset, news_info, dev_users, dev_user_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T02:57:19.715952Z",
     "start_time": "2022-04-26T02:30:09.651045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'LSTUR', 'epochs': 5, 'use_body': False, 'use_cate': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-04-26 10:35:03] [epoch 1] train_loss: 1.4230 valid_loss: 3.9809\n",
      "[2022-04-26 10:40:04] [epoch 2] train_loss: 1.3543 valid_loss: 4.0096\n",
      "[2022-04-26 10:45:04] [epoch 3] train_loss: 1.3303 valid_loss: 4.0102\n",
      "[2022-04-26 10:50:09] [epoch 4] train_loss: 1.3128 valid_loss: 4.0305\n",
      "[2022-04-26 10:55:14] [epoch 5] train_loss: 1.2993 valid_loss: 4.0550\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2fe6bd00164fa4a1b58d521d782c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] AUC: 0.668122, MRR: 0.312808, nDCG5:0.344502, nDCG10: 0.408948\n"
     ]
    }
   ],
   "source": [
    "args = {'model': 'LSTUR', 'epochs': 5,\n",
    "        'use_body': False,\n",
    "        'use_cate': False}\n",
    "print(args)\n",
    "\n",
    "model = Model(word_emb, cate_emb, args).to(device)\n",
    "train(model, train_dataset, valid_dataset, args['epochs'])\n",
    "evaluate(model, dev_dataset, news_info, dev_users, dev_user_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T01:40:38.866962Z",
     "start_time": "2022-04-26T00:48:28.558637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'LSTUR', 'epochs': 10, 'use_body': False, 'use_cate': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-04-26 08:53:22] [epoch 1] train_loss: 1.4882 valid_loss: 3.9070\n",
      "[2022-04-26 08:58:22] [epoch 2] train_loss: 1.4330 valid_loss: 3.9303\n",
      "[2022-04-26 09:03:22] [epoch 3] train_loss: 1.4038 valid_loss: 3.9207\n",
      "[2022-04-26 09:08:17] [epoch 4] train_loss: 1.3905 valid_loss: 3.9409\n",
      "[2022-04-26 09:13:22] [epoch 5] train_loss: 1.3801 valid_loss: 3.9727\n",
      "[2022-04-26 09:18:25] [epoch 6] train_loss: 1.3695 valid_loss: 3.9813\n",
      "[2022-04-26 09:23:31] [epoch 7] train_loss: 1.3600 valid_loss: 4.0153\n",
      "[2022-04-26 09:28:29] [epoch 8] train_loss: 1.3507 valid_loss: 4.0306\n",
      "[2022-04-26 09:33:25] [epoch 9] train_loss: 1.3420 valid_loss: 4.0551\n",
      "[2022-04-26 09:38:32] [epoch 10] train_loss: 1.3335 valid_loss: 4.0791\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168fb14223c74c338b08032126fcd938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] AUC: 0.635129, MRR: 0.294400, nDCG5:0.318585, nDCG10: 0.386345\n"
     ]
    }
   ],
   "source": [
    "args = {'model': 'LSTUR', 'epochs': 5,\n",
    "        'use_body': False,\n",
    "        'use_cate': False}\n",
    "print(args)\n",
    "\n",
    "model = Model(word_emb, cate_emb, args).to(device)\n",
    "train(model, train_dataset, valid_dataset, args['epochs'])\n",
    "evaluate(model, dev_dataset, news_info, dev_users, dev_user_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### title only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T13:27:09.116222Z",
     "start_time": "2022-04-25T13:07:15.186409Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'use_body': False, 'use_cate': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-04-25 21:11:38] [epoch 1] train_loss: 1.4365\n",
      "[2022-04-25 21:16:07] [epoch 2] train_loss: 1.3909\n",
      "[2022-04-25 21:20:34] [epoch 3] train_loss: 1.3749\n",
      "[2022-04-25 21:25:01] [epoch 4] train_loss: 1.3656\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0fa01c095554250a8cc9c747a13ef85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] AUC: 0.628971, MRR: 0.285745, nDCG5:0.312924, nDCG10: 0.379725\n"
     ]
    }
   ],
   "source": [
    "args = {'use_body': False,\n",
    "        'use_cate': False,}\n",
    "print(args)\n",
    "\n",
    "model = NAML(word_emb, cate_emb, args).to(device)\n",
    "train(model, train_dataset, None)\n",
    "evaluate(model, dev_dataset, news_info, dev_users, dev_user_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### title + category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T13:55:53.690978Z",
     "start_time": "2022-04-25T13:30:02.823039Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'use_body': False, 'use_cate': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-04-25 21:35:52] [epoch 1] train_loss: 1.4150 valid_loss: 4.0154\n",
      "[2022-04-25 21:41:42] [epoch 2] train_loss: 1.3703 valid_loss: 3.9589\n",
      "[2022-04-25 21:47:48] [epoch 3] train_loss: 1.3561 valid_loss: 3.9519\n",
      "[2022-04-25 21:53:45] [epoch 4] train_loss: 1.3466 valid_loss: 3.9338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a378554c2354a1ab570dddee9127e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] AUC: 0.660897, MRR: 0.311889, nDCG5:0.343381, nDCG10: 0.407997\n"
     ]
    }
   ],
   "source": [
    "args = {'model': 'NAML',\n",
    "        'use_body': False,\n",
    "        'use_cate': True,}\n",
    "print(args)\n",
    "\n",
    "model = NAML(word_emb, cate_emb, args).to(device)\n",
    "train(model, train_dataset, valid_dataset)\n",
    "evaluate(model, dev_dataset, news_info, dev_users, dev_user_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### title + abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T13:03:19.724783Z",
     "start_time": "2022-04-25T11:13:07.218501Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'use_body': True, 'use_cate': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-04-25 19:37:53] [epoch 1] train_loss: 1.4141 valid_loss: 3.8881\n",
      "[2022-04-25 20:02:43] [epoch 2] train_loss: 1.3622 valid_loss: 3.8662\n",
      "[2022-04-25 20:27:29] [epoch 3] train_loss: 1.3439 valid_loss: 3.8589\n",
      "[2022-04-25 20:52:11] [epoch 4] train_loss: 1.3319 valid_loss: 3.8762\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [264]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(args)\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m NAML(word_emb, cate_emb, args)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m evaluate(model, dev_dataset, news_info, dev_users, dev_user_hist)\n",
      "Input \u001b[0;32mIn [235]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset, valid_dataset, epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# torch.Size([16, 5, 30]) torch.Size([16, 50, 30]) torch.Size([16])\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m history \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(batch[\u001b[38;5;241m1\u001b[39m], dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong, device \u001b[38;5;241m=\u001b[39m device)\n\u001b[1;32m     15\u001b[0m correct \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(torch\u001b[38;5;241m.\u001b[39mtensor(batch[\u001b[38;5;241m2\u001b[39m], dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong, device \u001b[38;5;241m=\u001b[39m device), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = {'use_body': True,\n",
    "        'use_cate': False,}\n",
    "print(args)\n",
    "\n",
    "model = NAML(word_emb, cate_emb, args).to(device)\n",
    "train(model, train_dataset, valid_dataset)\n",
    "evaluate(model, dev_dataset, news_info, dev_users, dev_user_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T13:05:43.048758Z",
     "start_time": "2022-04-25T13:03:32.062375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49628396799e4b3fa53eed7a8c79bf7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] AUC: 0.647997, MRR: 0.305866, nDCG5:0.337099, nDCG10: 0.398425\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, dev_dataset, news_info, dev_users, dev_user_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### title + abstract + category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T09:25:50.057438Z",
     "start_time": "2022-04-25T07:58:57.018337Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'use_body': True, 'use_cate': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-04-25 16:20:29] [epoch 1] train_loss: 1.3996\n",
      "[2022-04-25 16:42:10] [epoch 2] train_loss: 1.3522\n",
      "[2022-04-25 17:03:59] [epoch 3] train_loss: 1.3354\n",
      "[2022-04-25 17:25:50] [epoch 4] train_loss: 1.3246\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "evaluate() takes 1 positional argument but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [256]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m NAML(word_emb, cate_emb, args)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m train(model, train_dataset, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnews_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_user_hist\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: evaluate() takes 1 positional argument but 5 were given"
     ]
    }
   ],
   "source": [
    "args = {'use_body': True,\n",
    "        'use_cate': True,}\n",
    "print(args)\n",
    "\n",
    "model = NAML(word_emb, cate_emb, args).to(device)\n",
    "train(model, train_dataset, None)\n",
    "evaluate(model, dev_dataset, news_info, dev_users, dev_user_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T09:29:38.049350Z",
     "start_time": "2022-04-25T09:27:28.670997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b5c98101aa4111ade91fa4af0d3ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] AUC: 0.645447, MRR: 0.302685, nDCG5:0.334674, nDCG10: 0.397407\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, dev_dataset, news_info, dev_users, dev_user_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "44c95cbe46b508eb9ccdabe43de284f7450b2dcd95f4efa956ef9c46f3314f5e"
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
